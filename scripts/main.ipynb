{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.mode.chained_assignment = None\n",
    "sns.set_theme()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo de posible metrica de desempeño\n",
    "## Para ejecucion de codigo ir a seccion Naive_Aproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(x,tolerance = 1.):    \n",
    "    return 1/ (1 + np.exp(-tolerance*x))\n",
    "def diff_normalization(x,num_pixel_tol = 100):\n",
    "    tolerance = (200/num_pixel_tol)/100\n",
    "    return np.tanh(tolerance * x)\n",
    "def diff_assignation(x): \n",
    "    return 1 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,10)\n",
    "y = diff_normalization(x,10)\n",
    "# print(y)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_table = pd.read_csv('etiquetas.csv')\n",
    "# example_table.head()\n",
    "\n",
    "new_label_table = pd.read_csv('renombramiento_etiquetas.csv')\n",
    "new_label_table = new_label_table.rename(columns = {'new_class' : 'class'})\n",
    "new_label_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_table.loc[new_label_table['class'] == 'Descripciones', 'class'] = 'Etiqueta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_table['xsize'] =  example_table['xmax'] - example_table['xmin']\n",
    "example_table['ysize'] =  example_table['ymax'] - example_table['ymin']\n",
    "print(example_table['xsize'].astype(float).mean() * 0.1 ,example_table['ysize'].astype(float).mean() * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstr = 'model_CNN_tensorflow.keras'\n",
    "sstr[:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculates the IoU (Intersection over Union) between two bounding boxes.\n",
    "    \n",
    "    Arguments:\n",
    "    box1 -- list or tuple containing [x1, y1, x2, y2] coordinates of the first bounding box\n",
    "    box2 -- list or tuple containing [x1, y1, x2, y2] coordinates of the second bounding box\n",
    "    \n",
    "    Returns:\n",
    "    iou -- float value representing the IoU between the two bounding boxes\n",
    "    \"\"\"\n",
    "    # calculate the area of each bounding box\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # calculate the coordinates of the intersection rectangle\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # calculate the area of the intersection rectangle\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # calculate the union of the two bounding boxes\n",
    "    union_area = area_box1 + area_box2 - intersection_area\n",
    "    \n",
    "    # calculate the IoU\n",
    "    iou = intersection_area / union_area\n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "def evaluate_object_detection(pred_boxes, true_boxes, iou_thresh):\n",
    "    \"\"\"\n",
    "    Computes the average precision for object detection using IoU metric.\n",
    "    \n",
    "    Args:\n",
    "    - pred_boxes (list of tuples): Predicted bounding boxes (x1, y1, x2, y2) for each object\n",
    "    - true_boxes (list of tuples): Ground truth bounding boxes (x1, y1, x2, y2) for each object\n",
    "    - iou_thresh (float): Threshold for determining a true positive detection\n",
    "    \n",
    "    Returns:\n",
    "    - ap (float): Average precision for the given set of predictions and ground truth boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary to store the number of true positives and false positives for each class\n",
    "    tp = {}\n",
    "    fp = {}\n",
    "    for i in range(len(pred_boxes)):\n",
    "        # Find the class of the predicted box (assuming all boxes have a class label)\n",
    "        cls = pred_boxes[i][4]\n",
    "        if cls not in tp:\n",
    "            tp[cls] = 0\n",
    "            fp[cls] = 0\n",
    "        \n",
    "        # Check if the predicted box overlaps with any of the ground truth boxes\n",
    "        overlaps = []\n",
    "        for j in range(len(true_boxes)):\n",
    "            overlap = calculate_iou(pred_boxes[i], true_boxes[j])\n",
    "            overlaps.append(overlap)\n",
    "        \n",
    "        # If there is at least one ground truth box with an IoU greater than the threshold, count it as a true positive\n",
    "        max_overlap = max(overlaps)\n",
    "        if max_overlap >= iou_thresh:\n",
    "            tp[cls] += 1\n",
    "        else:\n",
    "            fp[cls] += 1\n",
    "    \n",
    "    # Compute precision and recall for each class\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    for cls in tp:\n",
    "        precisions[cls] = tp[cls] / (tp[cls] + fp[cls])\n",
    "        recalls[cls] = tp[cls] / len([box for box in true_boxes if box[4] == cls])\n",
    "    \n",
    "    # Compute the average precision across all classes\n",
    "    ap = sum(precisions.values()) / len(precisions)\n",
    "    \n",
    "    return ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measuring_model_precision(table:pd.DataFrame,num_pixel_tol:int = 100):\n",
    "    \"\"\"vamos a calcular la precision del modelo pormedio de la evaluacion directa de la ubicacion de las cajas contenedoras generadas \n",
    "       y los labels asignados a dichas cajas, cada dato se encuentra apareado de manera apropiada.\n",
    "       \n",
    "       La evaluacion se ejecuta calculando la diferencia entre las coordenadas predichas por el modelo y las puestas a mano, esta diferencia se transforma\n",
    "       por medio de una funcion logistica \n",
    "\n",
    "    Args:\n",
    "        table (pd.DataFrame): tabla que aloja las posiciones de los labels de test y los arrojados por el modelo\n",
    "        tolerance (float)  : \n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    same_label = True if table['class'] == table['predicted_class'] else False\n",
    "    diff_posx = abs(table['xmin']-table['xmin_pred']) + abs(table['xmax']-table['xmax_pred'])\n",
    "    diff_posy = abs(table['ymin']-table['ymin_pred']) + abs(table['ymax']-table['ymax_pred'])\n",
    "    normalize_diff_posx = diff_normalization(diff_posx,num_pixel_tol = num_pixel_tol)\n",
    "    normalize_diff_posy = diff_normalization(diff_posy,num_pixel_tol = num_pixel_tol)\n",
    "    # score += diff_assignation(normalize_diff_posx) / 3\n",
    "    # score += diff_assignation(normalize_diff_posy) / 3\n",
    "    box_labeled = [table['xmin'],table['xmax'],table['ymin'],table['ymax']]\n",
    "    box_pred = [table['xmin_pred'],table['xmax_pred'],table['ymin_pred'],table['ymax_pred']]\n",
    "    score += evaluate_object_detection(box_labeled, box_pred, num_pixel_tol) / 2\n",
    "    if same_label:\n",
    "        score += 1 / 2\n",
    "    table['score'] = score\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drive.MyDrive.tesis.make_cfg import SAMPLE_DICT,make_cfg_file\n",
    "\n",
    "# pytorc_config_values = [[3,3,3,3,3],[8,16,32,64,128],['same','same','same','same','same'],['relu','relu','relu','relu','relu'],[2,2,2,2,2],[3,3,3,3,3],['valid','valid','valid','valid','valid']]\n",
    "# new_dict_pytorc_config_values = {}\n",
    "# for k,nv in zip(SAMPLE_DICT.keys(),pytorc_config_values):\n",
    "#     new_dict_pytorc_config_values[k] = nv\n",
    "#     print(nv)\n",
    "\n",
    "new_dict_pytorc_config_values =  {\n",
    "                        'kernel_size_conv': [8,16,32,64], #, 2, 2, 2, 2\n",
    "                        'filters': [3,3,3,3], #,16, 32, 64,128\n",
    "                        'conv_padding': [2,2,2,2], #,1,1,1,1\n",
    "                        'activation': ['relu','relu','relu','relu'], #, 'relu', 'relu', 'relu', 'relu'\n",
    "                        'pool_size': [4,4,4,4], #, 2, 2, 2, 2\n",
    "                        'conv_stride': [1,1,1,1], #, 1, 1, 1, 1\n",
    "                        'pool_stride': [3,3,3,3], #, 3, 3, 3, 3\n",
    "                        'pool_padding': [2,2,2,2], #,1,1,1,1\n",
    "                        'padding_mode_conv' : ['zeros','zeros','zeros','zeros'], # ,'zeros','zeros','zeros','zeros' zeros', 'reflect', 'replicate' or 'circular'\n",
    "\n",
    "                        }\n",
    "\n",
    "\n",
    "\n",
    "make_cfg_file(new_dict_pytorc_config_values,'torch_simple.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def contar_clases_dataset(table:pd.DataFrame):\n",
    "    \"\"\"crea una colunma llamada class_count con la cantidad de etiquetas diferentes en la imagen\n",
    "\n",
    "    Args:\n",
    "        table (pd.DataFrame): tabla de datos de las imagenes\n",
    "    \"\"\"\n",
    "    # table['class_count'] = ''\n",
    "    l = {}\n",
    "    for label in table.class_name.unique():\n",
    "        table_filter = table.loc[table.class_name == label]\n",
    "        labelCounr = table_filter.shape[0]\n",
    "        totalLabels = table.shape[0]\n",
    "        l[label] = labelCounr/totalLabels\n",
    "    \n",
    "    return l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.paths import get_project_labels\n",
    "unLabels = get_project_labels('etiquetas.csv')\n",
    "MELULabels = get_project_labels('etiquetas_MELU.csv')\n",
    "unLabelsTable = pd.read_csv(unLabels)\n",
    "MELULabelsTable = pd.read_csv(MELULabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = contar_clases_dataset(unLabelsTable)\n",
    "l2 = contar_clases_dataset(MELULabelsTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(l1.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_labels_distribution(table:pd.DataFrame):\n",
    "    \"\"\"dibuja la distribucion de las etiquetas en la tabla\n",
    "\n",
    "    Args:\n",
    "        table (pd.DataFrame): tabla de datos de las imagenes\n",
    "    \"\"\"\n",
    "    l = contar_clases_dataset(table)\n",
    "    plt.bar(l.keys(),l.values())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.DataFrame.from_dict(l1,orient='index').reset_index().rename(columns = {'index':'class',0:'percentage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_dict(l2,orient='index').reset_index().rename(columns = {'index':'class',0:'percentage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns   \n",
    "print(list(sns.color_palette('husl', 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "palette = sns.color_palette('Dark2', 10)\n",
    "# Create the histogram plot\n",
    "ax = sns.barplot(data=df1,  palette=palette, x = 'class',hue='class', y='percentage')\n",
    "# ax.set(title=\"set datos un\")\n",
    "ax.set(xlabel='Class', ylabel='Percentage')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ax = sns.barplot(data=df2,  palette=palette, x = 'class',hue='class', y='percentage')\n",
    "ax.set(title=\"set datos MELU\")\n",
    "ax.set(xlabel='clases', ylabel='porcentaje')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_evaluator import PytorchEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_balance_table_compilation(table:pd.DataFrame, fraction :float = 0.6):\n",
    "    \"\"\"\" Genera una tabla de imagenes con sus etiquetas y posiciones de tal manera que el conjunto resultante sea el mas balanceado posible \n",
    "    en terminos de tipos y numeros de etiquetas\n",
    "\n",
    "    Args:\n",
    "        table (pd.tableFrame): tabla de imagenes y etiquetas totales\n",
    "    \"\"\"\n",
    "    tabla_contada = contar_clases_por_imagen(table)\n",
    "    tabla_contada['class_ratio'] = tabla_contada['class_count'] / tabla_contada['class_count'].max()\n",
    "    # print(tabla_contada.loc[tabla_contada.class_count == tabla_contada.label_count])\n",
    "    # print(tabla_contada[tabla_contada.class_ratio.ge(1)])\n",
    "    tabla_contada.loc[tabla_contada.class_count == tabla_contada.label_count, 'class_ratio'] += tabla_contada['label_count'] / tabla_contada['class_count'].max()\n",
    "    tabla_contada.loc[tabla_contada.class_count.lt(tabla_contada.label_count * 0.66), 'class_ratio'] = 0 #tabla_contada['label_count'] / tabla_contada['class_count'].max()\n",
    "    tabla_contada_simplificada = tabla_contada.drop_duplicates(subset = 'filename').reset_index()\n",
    "    # print(tabla_contada_simplificada[tabla_contada_simplificada.class_ratio.ge(1)])\n",
    "    train_sample = tabla_contada_simplificada.sample(weights = tabla_contada_simplificada['class_ratio'],frac = fraction)\n",
    "    # print(train_sample[train_sample.class_ratio.ge(1)])\n",
    "    table_train_sample = table.loc[table.filename.isin(train_sample.filename)].reset_index(drop = True)\n",
    "    table_test_sample = table.loc[~table.filename.isin(train_sample.filename)].reset_index(drop = True)\n",
    "    return table_train_sample,table_test_sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set = train_balance_table_compilation(new_label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = train_set, y = \"class\").set(title = \"set entremamiento\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = test_set, y = \"class\").set(title = \"set testeo\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meludata = pd.read_csv('etiquetas_MELU.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = new_label_table, y = \"class\").set(title = \"set datos originales\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Aproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_number_according_to_class(series : pd.Series):\n",
    "    hash_dict = {}\n",
    "    for i,label in enumerate(series.unique()):\n",
    "        hash_dict[label] = i\n",
    "    return hash_dict\n",
    "\n",
    "def put_hash_column(table : pd.DataFrame):\n",
    "    hash_dict = assign_number_according_to_class(table['class_name'])\n",
    "    for k,v in hash_dict.items():\n",
    "        table.loc[table.class_name == k,'class_hash'] = int(v)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo txt annotation creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_table = pd.read_csv('etiquetas.csv')\n",
    "data_table = data_table.drop(columns = 'valid_path')\n",
    "data_table = put_hash_column(data_table)\n",
    "\n",
    "yolo_annotation_folder = os.path.join(os.getcwd(),'data','labels')\n",
    "\n",
    "if not os.path.exists(yolo_annotation_folder):\n",
    "    os.makedirs(yolo_annotation_folder)\n",
    "\n",
    "annotations_grouped_by_image = data_table.groupby(\"filename\")\n",
    "\n",
    "for i,file_table in annotations_grouped_by_image:\n",
    "    with open(f\"{os.path.join(yolo_annotation_folder,i[:-4])}.txt\",\"w+\") as file:\n",
    "        for j,row in file_table.iterrows():\n",
    "            class_width = row[\"xmax\"]-row[\"xmin\"]\n",
    "            class_height = row[\"ymax\"]-row[\"ymin\"]\n",
    "            line = f'{int(row[\"class_hash\"])} {(row[\"xmin\"] + (0.5 * class_width))/row[\"width\"]} {(row[\"ymin\"] + (0.5 * class_height))/row[\"height\"]} {class_width/row[\"width\"]} {class_height/row[\"height\"]}\\n'\n",
    "            file.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMACION DE DATOS A COCO FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List \n",
    "import functools\n",
    "\n",
    "@dataclass \n",
    "class JsonCOCOFormatter:\n",
    "    \n",
    "    ##### creacion de formato\n",
    "    info: Dict = field(init = False, default_factory =  dict),\n",
    "    licenses: List[Dict] = field(default_factory = list),\n",
    "    images: List[Dict] = field(default_factory = list),\n",
    "    annotations: List[Dict] = field(default_factory = list),\n",
    "    categories: List[Dict] = field(default_factory = list), \n",
    "    # segment_info: List[Dict] = field(default_factory = list), \n",
    "    \n",
    "    ##### input variables\n",
    "    url : str = field(default=\"None\")\n",
    "    id_lincense : int = field(default = None)\n",
    "\n",
    "    lincense_name : str = field(default = \"generic_lincense\")\n",
    "    file_name : str = field(default = \"\")\n",
    "    coco_url : str = field(default = \"None\")\n",
    "    height : int = field(default = None)\n",
    "    width : int = field(default = None)\n",
    "    date_captured : str = field(default = \"None\")\n",
    "    flickr_url : str = field(default = \"None\")\n",
    "    id_photo : int = field(default = None)\n",
    " \n",
    "    category_name : str = field(default = \"\")\n",
    "    subcategory_name : str = field(default = \"\")\n",
    "    id_category : int = field(default = None)\n",
    " \n",
    "    segmentation : List[int] = field(default_factory = list)\n",
    "    is_crowd : str = field(default = \"FALSE\")\n",
    "    bbox : List[int] = field(default_factory = list)\n",
    "    id_annotation : int = field(default = None)\n",
    "    SAVE_PATH : str = field(default = os.path.join(os.getcwd(),'annotations'))\n",
    "    FILE_NAME : str = field(default = 'dataset.json')\n",
    "   \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.info = {\n",
    "                    \"year\": \"2023\",\n",
    "                    \"version\": \"1.0\",\n",
    "                    \"description\": \"National Herbarium DataSet\",\n",
    "                    \"contributor\": \"DataLab Universidad Nacional de Colombia\",\n",
    "                    \"url\": \"\",\n",
    "                    \"date_created\": \"2023/04/20\"\n",
    "                    }\n",
    "    \n",
    "    def make_lincense(self):\n",
    "        return self.licenses.append({\n",
    "                \"url\": self.url,\n",
    "                \"id\": self.id_lincense,\n",
    "                \"name\": self.lincense_name\n",
    "            })\n",
    "        \n",
    "    def make_image(self):\n",
    "        condition = True\n",
    "        for x in self.images:\n",
    "            if self.file_name in x.values():\n",
    "                condition = False\n",
    "        \n",
    "        if condition:\n",
    "            return self.images.append({\n",
    "                \"license\": self.id_lincense,\n",
    "                \"file_name\": self.file_name,\n",
    "                \"coco_url\": self.coco_url,\n",
    "                \"height\": self.height,\n",
    "                \"width\": self.width,\n",
    "                \"date_captured\": self.date_captured,\n",
    "                \"flickr_url\": self.flickr_url,\n",
    "                \"id\": self.id_photo,\n",
    "                # \"annotations\" : self.annotations\n",
    "                })\n",
    "        \n",
    "    def make_category(self):\n",
    "        condition = True\n",
    "        for x in self.categories:\n",
    "            if self.subcategory_name in x.values():\n",
    "                condition = False\n",
    "        if condition:\n",
    "            return self.categories.append({\n",
    "                    \"supercategory\": self.category_name,\n",
    "                    \"id\": self.id_category,\n",
    "                    \"name\": self.subcategory_name\n",
    "                })\n",
    "            \n",
    "    def make_annotation(self):\n",
    "        return self.annotations.append({\n",
    "                \"segmentation\": self.segmentation,\n",
    "                \"area\": self.height * self.width,\n",
    "                \"iscrowd\": self.is_crowd,\n",
    "                \"image_id\": self.id_photo,\n",
    "                \"bbox\": self.bbox,\n",
    "                \"category_id\": self.id_category,\n",
    "                \"id\": self.id_annotation\n",
    "            })\n",
    "            \n",
    "    def to_file(self):\n",
    "        target_keys =['info','licenses','images','annotations','categories']\n",
    "        target_dict = {key: value for key, value in self.__dict__.items() if key in target_keys}\n",
    "        if not os.path.exists(self.SAVE_PATH):\n",
    "            os.makedirs(self.SAVE_PATH)\n",
    "        with open(os.path.join(self.SAVE_PATH,self.FILE_NAME),'w+') as file:\n",
    "            # file.write(str(target_dict).replace('\"',\"'\").strip('\"<>()'))\n",
    "            file.write(str(target_dict).replace(\"'\",'\"').strip(\"'<>()\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = pd.read_csv('etiquetas.csv')\n",
    "data_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = pd.read_csv('etiquetas.csv')\n",
    "data_table = data_table.drop(columns = 'valid_path')\n",
    "data_table = put_hash_column(data_table)\n",
    "\n",
    "def bbox_COCO_format(bbox:list[float])->list[float]:\n",
    "    x_center = 0.5 * (bbox[0] + bbox[2])\n",
    "    y_center = 0.5 * (bbox[1] + bbox[3])\n",
    "    width = np.abs(bbox[2] - bbox[0])\n",
    "    height = np.abs(bbox[3] - bbox[1])\n",
    "    return [x_center,y_center,width,height]\n",
    "\n",
    "def bbox_de_COCO_format(bbox:list[float])->list[float]:\n",
    "    x_min = bbox[0] - (0.5 * bbox[2])\n",
    "    y_min = bbox[1] - (0.5 * bbox[3])\n",
    "    x_max = x_min + bbox[2]\n",
    "    y_max = y_min + bbox[3]\n",
    "    return [x_min,y_min,x_max,y_max]\n",
    "\n",
    "file_information_gather = data_table.groupby('filename')\n",
    "id_picture = 0\n",
    "id_licencia = 0\n",
    "imagenes , licensias, anotaciones, categorias = [],[],[],[]\n",
    "for picture_name, sub_table in file_information_gather:\n",
    "    id_picture += 1\n",
    "    id_licencia += 1\n",
    "    id_anotacion = 0\n",
    "    picture_path = os.path.abspath(picture_name) #os.path.join(os.getcwd(),'data','imagenes', picture_name)\n",
    "    single_photo_data = functools.partial(JsonCOCOFormatter,images=imagenes,licenses=licensias,annotations=anotaciones,categories=categorias,file_name=picture_path,height=sub_table.height.unique()[0],width=sub_table.width.unique()[0],id_lincense = id_licencia, id_photo = id_picture)\n",
    "    for i,row in sub_table.iterrows():\n",
    "        id_anotacion += 1\n",
    "        bbox_paste = bbox_COCO_format([row[\"xmin\"],row[\"ymin\"],row['xmax'],row['ymax']]) # coordenadas para dar puntos medios y tamaños de las cajas[(row[\"xmin\"] + (0.5 * class_width))/row[\"width\"],(row[\"ymin\"] + (0.5 * class_height))/row[\"height\"],class_width/row[\"width\"],class_height/row[\"height\"]]#\n",
    "        whole_data = single_photo_data(category_name = \"etiquetas\", subcategory_name = row['class_name'],segmentation = [\"empty\"],bbox = bbox_paste,id_annotation=id_anotacion,id_category=row['class_hash'])\n",
    "        whole_data.make_lincense()\n",
    "        whole_data.make_category()\n",
    "        whole_data.make_annotation()\n",
    "        whole_data.make_image()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data.to_file()#.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.print(physical_devices)\n",
    "\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJEMPLO DE CREACION NUEVA ARQUITECTURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_cfg import SAMPLE_DICT,make_cfg_file\n",
    "\n",
    "new_arch_values = [[64,192,128,512,1024],[7,3,3,3,3],['same','same','same','same','same'],['relu','relu','relu','relu','relu'],[2,2,2,2,2],[None,None,None,None,None],['valid','valid','valid','valid','valid']]\n",
    "new_dict_arch = {}\n",
    "for k,nv in zip(SAMPLE_DICT.keys(),new_arch_values):\n",
    "    new_dict_arch[k] = nv\n",
    "\n",
    "make_cfg_file(new_dict_arch,'config/yolo_simple','json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cfg_file(SAMPLE_DICT,'config/default_config','json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm config/torch_simple.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_cfg import SAMPLE_DICT,make_cfg_file\n",
    "\n",
    "pytorc_config_values = [[3,3,3,3,3],[128,64,32,16,8],['valid','valid','valid','valid','valid'],['relu','relu','relu','relu','relu'],[2,2,2,2,2],[None,None,None,None,None],['valid','valid','valid','valid','valid']]\n",
    "new_dict_pytorc_config_values = {}\n",
    "for k,nv in zip(SAMPLE_DICT.keys(),pytorc_config_values):\n",
    "    new_dict_pytorc_config_values[k] = nv\n",
    "\n",
    "make_cfg_file(new_dict_pytorc_config_values,'config/torch_simple.cfg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinEntriesOfList(lista:list,juntionSize:int = 2):\n",
    "    jointList = []\n",
    "    for leftSide in range(0,len(lista),juntionSize):\n",
    "        rightSide = leftSide + juntionSize\n",
    "        jointList.append(lista[leftSide:rightSide])\n",
    "    return jointList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [x for x in range(100)]\n",
    "l2 = joinEntriesOfList(l1,3)\n",
    "l3 = joinEntriesOfList(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python naive_model_torch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecucion de codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python naive_model.py --batch_size 50 --epochs 200 --model_structure config/yolo_simple.cfg --save_model YoloSimpBatch50Epochs200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecucion arquitectura default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python naive_model.py --batch_size 50 --device 'GPU' --epochs 200 --save_model BaseEpoch200Batch50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lectura de modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from models import CustomIoUMetric\n",
    "custom_objects = {\"CustomIoUMetric\": CustomIoUMetric}\n",
    "\n",
    "scanned_model = tf.keras.models.load_model('models/CNN_modeltest1',custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from typing import Callable\n",
    "# read image\n",
    "from utils.paths import get_project_data_MELU_dir, get_project_data_UN_dir,get_project_results\n",
    "\n",
    "\n",
    "path1 = get_project_data_UN_dir('imagenes')\n",
    "path2 = get_project_data_MELU_dir('trian/images')\n",
    "\n",
    "def monochromaticIntensityHistogram(path : str,save:bool = False,show:bool = False):\n",
    "    imageList = os.listdir(path)\n",
    "    savePath = get_project_results(f'images/histograms/')\n",
    "    os.makedirs(savePath,exist_ok=True)\n",
    "    for image in imageList[:5]:\n",
    "        im = cv2.imread(os.path.join(path,image))\n",
    "        vals = im.mean(axis=2).flatten()\n",
    "        counts, bins = np.histogram(vals, range(257))\n",
    "        plt.bar(bins[:-1] - 0.5, counts, width=1, edgecolor='none')\n",
    "        plt.xlim([-0.5, 255.5])\n",
    "        plt.title('Histogram for intensity (grey scale) picture')\n",
    "        plt.xlabel('Intensity value')\n",
    "        plt.ylabel('Pixel Count')\n",
    "        if save:\n",
    "            plt.savefig(os.path.join(savePath,f'{image[:-4]}_histogram'),format = 'png')\n",
    "            with open(os.path.join(savePath,f'{image[:-4]}_histogram.txt'),'w+') as file:\n",
    "                file.write(str({'bins':bins,'counts':counts}))\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.graphics import monochromaticIntensityHistogram,RGBhitogram\n",
    "from utils.paths import get_project_data_MELU_dir, get_project_data_UN_dir,get_project_results\n",
    "\n",
    "path1 = get_project_data_UN_dir('imagenes')\n",
    "path2 = get_project_data_MELU_dir('trian/images')\n",
    "# monochromaticIntensityHistogram(path=path1,save=True,show=False)\n",
    "RGBhitogram(path=path1,save=True,show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monochromaticIntensityHistogram(path1,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "image_un = get_project_data_UN_dir('imagenes')\n",
    "imageList = os.listdir(image_un)\n",
    "for image in imageList[:5]:\n",
    "    img = cv2.imread(get_project_data_UN_dir(f'imagenes/{image}'))\n",
    "    color = ('b','g','r')\n",
    "    for i,col in enumerate(color):\n",
    "        histr = cv.calcHist([img],[i],None,[256],[0,256])\n",
    "        plt.plot(histr,color = col)\n",
    "        plt.xlim([0,256])\n",
    "   \n",
    "    plt.title('Histogram for color scale picture')\n",
    "    plt.show()\n",
    "\n",
    "# while True:\n",
    "#     k = cv2.waitKey(0) & 0xFF     \n",
    "#     if k == 27: break  \n",
    "# cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_maximun_number_of_annotation_in_set, train_batch_consolidation, read_data, image_test_set, image_train_set\n",
    "import pickle\n",
    "\n",
    "\n",
    "IMG_SHAPE = (500, 500)\n",
    "total_test_images = []\n",
    "total_test_targets = []\n",
    "\n",
    "images,annotations,_,_ = read_data()\n",
    "\n",
    "test_bites = open('sets/test_image_set','rb')\n",
    "test_images = pickle.load(test_bites)#image_test_set(images,train_images)\n",
    "\n",
    "max_n_boxes_test = get_maximun_number_of_annotation_in_set(annotations,images)\n",
    "images_for_test , test_targets = train_batch_consolidation(test_images,total_test_images,annotations,total_test_targets,max_n_boxes_test,IMG_SHAPE)\n",
    "images_for_test = tf.cast(images_for_test, dtype=tf.float32)\n",
    "evaluations = scanned_model.evaluate(images_for_test,test_targets,batch_size=len(images_for_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MELUB102930a_sp64414160451423553680_medium image\n",
    "MELUA002114a_sp64023357247815722155_medium annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def display_yaml_content(yaml_file:yaml,return_data:bool = True)->dict:\n",
    "    with open(yaml_file) as file:\n",
    "        documents = yaml.full_load(file)\n",
    "        for item,doc in documents.items():\n",
    "            print(item,':',doc)\n",
    "    return documents if return_data else None \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 small database label\n",
    "1 handwritten data\n",
    "2 stamp\n",
    "3 annotation label\n",
    "4 scale\n",
    "5 swing tag\n",
    "6 full database label\n",
    "7 database label\n",
    "8 swatch\n",
    "9 institutional label\n",
    "10 number(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confg_yaml(train_path:str,validation_path:str,categories:dict,config_file_name:str = 'default')->None:\n",
    "    '''\n",
    "    creates a configuration file for the ultralytics yolo package, recives the path to training\n",
    "    and validation images along with a dictionary containing the desired target classes\n",
    "    '''\n",
    "\n",
    "    config_dict = {\n",
    "        'train':train_path,\n",
    "        'val': validation_path,\n",
    "        'names' : categories\n",
    "        }\n",
    "    \n",
    "    with open(f'{config_file_name}.yaml' , 'w') as config_file:\n",
    "        yaml.dump(config_dict,config_file,default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "coco_file_path = os.path.join(os.getcwd(),'annotations','dataset.json')\n",
    "coco_file = open(coco_file_path, 'r')\n",
    "coco_file_open = json.load(coco_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_file_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "dt.datetime.now().month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from drive.MyDrive.tesis.COCO_formatter import bbox_de_COCO_format,bbox_COCO_format\n",
    "from cv2 import imread, imshow, rectangle,putText,FONT_HERSHEY_SIMPLEX\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "def draw_bbox(image_path:str,ids:list[int],coordinates:list[list[float]],cloud:bool=False):\n",
    "    image_to_visualize = image_path #os.path.join(IMG_MEL_PATH,image_max_name)\n",
    "    img = imread(image_to_visualize)\n",
    "    for id,coordinate in zip(ids,coordinates) :\n",
    "        nc = bbox_de_COCO_format([float(x.replace('\\n','')) for x in coordinate])\n",
    "        rectangle(img,(int(nc[0]*img.shape[1]),int(nc[1]*img.shape[0])),((int(nc[2]*img.shape[1]),int(nc[3]*img.shape[0]))),(255,0,0),3)\n",
    "        putText(img, id, (int(nc[0]*img.shape[1])-20,int(nc[1]*img.shape[0])+10), FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)\n",
    "    if cloud : cv2_imshow(img)\n",
    "    else : imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_cfg import make_cfg_file\n",
    "\n",
    "new_dict_pytorc_config_values =  params = {\n",
    "                                    'kernel_size_conv': [3, 3, 3, 3],\n",
    "                                    'filters': [3, 3, 3, 3],  # Changed the first filter value from 3 to 16\n",
    "                                    'conv_padding': [2, 2, 2, 2],\n",
    "                                    'activation': ['relu', 'relu', 'relu', 'relu'],\n",
    "                                    'pool_size': [4, 4, 4, 4],\n",
    "                                    'conv_stride': [1, 1, 1, 1],\n",
    "                                    'pool_stride': [3, 3, 3, 3],\n",
    "                                    'pool_padding': [2, 2, 2, 2],\n",
    "                                    'padding_mode_conv': ['zeros', 'zeros', 'zeros', 'zeros']\n",
    "                                }\n",
    "\n",
    "#{\n",
    "#                         'kernel_size_conv': [8,16,32,64], #, 2, 2, 2, 2\n",
    "#                         'filters': [3,3,3,3], #,16, 32, 64,128\n",
    "#                         'conv_padding': [2,2,2,2], #,1,1,1,1\n",
    "#                         'activation': ['relu','relu','relu','relu'], #, 'relu', 'relu', 'relu', 'relu'\n",
    "#                         'pool_size': [4,4,4,4], #, 2, 2, 2, 2\n",
    "#                         'conv_stride': [1,1,1,1], #, 1, 1, 1, 1\n",
    "#                         'pool_stride': [3,3,3,3], #, 3, 3, 3, 3\n",
    "#                         'pool_padding': [2,2,2,2], #,1,1,1,1\n",
    "#                         'padding_mode_conv' : ['zeros','zeros','zeros','zeros'], # ,'zeros','zeros','zeros','zeros' zeros', 'reflect', 'replicate' or 'circular'\n",
    "\n",
    "#                         }\n",
    "\n",
    "\n",
    "\n",
    "make_cfg_file(new_dict_pytorc_config_values,'config/torch_simple','json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm config/torch_simple.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python COCO_formatter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm annotations/dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "#coco_file_path = os.path.join(os.getcwd(),'annotations','dataset.json')\n",
    "coco_file = open('config/torch_simple.json', 'r')\n",
    "coco_file_open = json.load(coco_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_file_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in coco_file_open.values():\n",
    "    [print(type(x)) for x in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import imread\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "base_path = '/Users/juanpablovargasacosta/herbario/data/imagenes/'\n",
    "all_means = []\n",
    "all_stds = []\n",
    "for file in os.listdir(base_path):\n",
    "    if file.endswith('.png'):\n",
    "        image = imread(os.path.join(base_path,file))\n",
    "        means_by_channels = np.mean(image,axis=(0,1))\n",
    "        stds_by_channels = np.std(image,axis=(0,1))\n",
    "        all_means.append(means_by_channels)\n",
    "        all_stds.append(stds_by_channels)\n",
    "\n",
    "arrange_means = np.stack(all_means)\n",
    "arrange_stds = np.stack(all_stds)\n",
    "\n",
    "print(np.mean(arrange_means,axis=0))\n",
    "print(np.mean(arrange_stds,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std_per_channel(image_folder:str)->list[float]:\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "    for file in os.listdir(image_folder):\n",
    "        if file.endswith('.png'):\n",
    "            image = imread(os.path.join(image_folder,file))\n",
    "            means_by_channels = np.mean(image,axis=(0,1))\n",
    "            stds_by_channels = np.std(image,axis=(0,1))\n",
    "            all_means.append(means_by_channels)\n",
    "            all_stds.append(stds_by_channels)\n",
    "\n",
    "    arrange_means = np.stack(all_means)\n",
    "    arrange_stds = np.stack(all_stds)\n",
    "\n",
    "    return [np.mean(arrange_means,axis=0),np.mean(arrange_stds,axis=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = calculate_mean_std_per_channel('/Users/juanpablovargasacosta/herbario/data/imagenes/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cache_file_bytes(fileName:str)->bytes:\n",
    "    with open(fileName,'rb') as file:\n",
    "        contenido = file.read()\n",
    "    return contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_cache_file_json(fileName:str):\n",
    "    \"\"\"Reads the content of the cache file with the data of the last execution.\n",
    "\n",
    "    :returns: tuple with boolean and a datetime if file is found. True, None if any exception is raised\n",
    "    \"\"\"\n",
    "    default_output = True, None\n",
    "    try:\n",
    "        with open(fileName, 'r', encoding='ISO-8859-1') as f:\n",
    "            cache = json.load(f)\n",
    "            return cache\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('file not found...')\n",
    "        return default_output\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print('file could not be decoded...')\n",
    "        return default_output\n",
    "    except KeyError:\n",
    "        print('key not found...')\n",
    "        return default_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_cache_file_numpy(cache_filename, image_IDs=None):\n",
    "    \"\"\" Returns image data from a cache file\n",
    "\n",
    "    If ``image_IDs`` is not None this function will try to ensure data from\n",
    "    cache file come from the list of image IDs provided. If cache file does not\n",
    "    contain a list of image IDs, it will skip the check and return cache data.\n",
    "\n",
    "    Args:\n",
    "        cache_filename (str): cache filename\n",
    "        image_IDs (iterable, optional): list of image IDs corresponding to data\n",
    "            in cache file. If not specified, function will not check for\n",
    "            correspondence (default: None)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray, or None: Return Y as np.ndarray if possible and if the\n",
    "            cache file passes the consistency check specified by ``image_IDs``,\n",
    "            else None\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cache = np.load(cache_filename)\n",
    "    except IOError:\n",
    "        return None\n",
    "    \n",
    "    if _image_ID_str in cache.files and image_IDs is not None:\n",
    "        if not np.array_equal(image_IDs, cache[_image_ID_str]):\n",
    "            logger.warning('Cache file data in {f} do not match images '\n",
    "                           'specified'.format(f=cache_filename))\n",
    "            return None\n",
    "\n",
    "    return cache['Y'] if 'Y' in cache.files else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "basePath = '/Volumes/ADATA HD680/Shared/Files From d.localized/Maestria/tesis/data/train'\n",
    "cacheFile = os.path.join(basePath,'imagenes.cache')\n",
    "imageFolder = os.path.join(basePath,'images')\n",
    "labelsFolder = os.path.join(basePath,'labels')\n",
    "\n",
    "\n",
    "imageList = os.listdir(imageFolder)\n",
    "labelList = os.listdir(labelsFolder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creacion de csv a partir de txt de MELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import yaml\n",
    "from utils_tensorflow import dict_load,txt_file_information_colector\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('config_melu.yaml','r') as file:\n",
    "    file_config_melu = yaml.load(file,Loader=yaml.FullLoader)\n",
    "labels_melu = file_config_melu['names']\n",
    "# print(labels_melu)\n",
    "\n",
    "total_dict_img = []\n",
    "for image in tqdm(imageList,total=len(imageList)):\n",
    "    total_image_dict = {}\n",
    "    im = Image.open(os.path.join(imageFolder,image))\n",
    "    size = im.size\n",
    "\n",
    "    text_file_info = txt_file_information_colector(os.path.join(labelsFolder,image[:-4]+'.txt'))\n",
    "    new_keys = ['class_name','xmin','ymin','xmax','ymax']\n",
    "    for key,value in zip(new_keys,text_file_info.values()):\n",
    "        \n",
    "        for i in range(len(value)):\n",
    "            dict_load(total_image_dict,value[i],key)\n",
    "            dict_load(total_image_dict,image,'filename')\n",
    "            dict_load(total_image_dict,size[0],'width')\n",
    "            dict_load(total_image_dict,size[1],'height')\n",
    "  \n",
    "    \n",
    "    total_image_dict['class_name'] = list(map(lambda x: labels_melu[int(x)],total_image_dict['class_name']))\n",
    "    \n",
    "    df_single_image = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in total_image_dict.items() ]))\n",
    "    total_dict_img.append(df_single_image.dropna())\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_melu_ann = pd.concat(total_dict_img,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_melu_ann.to_csv('etiquetas_melu.csv',index=False)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "total_melu_ann = pd.read_csv('etiquetas_melu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from COCO_formatter import main\n",
    "main(total_melu_ann,fileName='dataset_melu.json',cocoFormat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notFoundImages = []\n",
    "for image in imageList:\n",
    "    if image[:-4] + '.txt' not in labelList:\n",
    "        notFoundImages.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageNoLabelFolder = os.path.join(basePath,'imagesNoLabels')\n",
    "labelsNoImageFolder = os.path.join(basePath,'labelsNoImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNotFound = []\n",
    "for label in labelList:\n",
    "    if label[:-4] + '.png' not in imageList:\n",
    "        labelsNotFound.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelsNotFound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_tensorflow import bbox_COCO_format,txt_file_information_colector\n",
    "from tqdm import tqdm \n",
    "for label in tqdm(labelList,total=len(labelList)):\n",
    "    labelFile = os.path.join(labelsFolder,label)\n",
    "\n",
    "    labelDict = txt_file_information_colector(labelFile)\n",
    "    v1n,v2n,v3n,v4n = [],[],[],[]\n",
    "\n",
    "    for j in range(len(labelDict['categories'])):\n",
    "        annotationRow = [labelDict[key][j] for key in labelDict.keys() if key != 'categories']\n",
    "        transformAnnotationRow = bbox_COCO_format(annotationRow)\n",
    "        v1n.append(transformAnnotationRow[0])\n",
    "        v2n.append(transformAnnotationRow[1])\n",
    "        v3n.append(transformAnnotationRow[2])\n",
    "        v4n.append(transformAnnotationRow[3])\n",
    "    \n",
    "    labelDict.update({'v1':v1n,'v2':v2n,'v3':v3n,'v4':v4n}) \n",
    "\n",
    "    with open(labelFile,'w') as file:\n",
    "        for i in range(len(labelDict['categories'])):\n",
    "            file.write(f\"{labelDict['categories'][i]} {labelDict['v1'][i]} {labelDict['v2'][i]} {labelDict['v3'][i]} {labelDict['v4'][i]}\\n\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nlabel in labelsNotFound:\n",
    "    if nlabel[:-4] + '.png' in notFoundImages:\n",
    "        print(nlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nimage in notFoundImages:\n",
    "    os.replace(os.path.join(imageFolder,nimage),os.path.join(imageNoLabelFolder,nimage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(labelsNoImageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nlabel in labelsNotFound:\n",
    "    os.replace(os.path.join(labelsFolder,nlabel),os.path.join(labelsNoImageFolder,nlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labelsNotFound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = os.listdir(labelsFolder)\n",
    "len(labelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_tensorflow import confg_yaml\n",
    "import json\n",
    "import os\n",
    "#coco_file_path = os.path.join(os.getcwd(),'annotations','dataset.json')\n",
    "coco_file = open('config/torch_simple.json', 'r')\n",
    "coco_file_open = json.load(coco_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/juanpablovargasacosta/herbario/data/imagenes/'\n",
    "confg_yaml(base_path,base_path,documents['names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,value in zip(scanned_model.metrics_names,evaluations) :\n",
    "    print(f'metric name : {name} - value : {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metricas usando sigmoide para bbox normalizadas y softmax para clases\n",
    "\n",
    "metric name : loss - value : 0.14827848970890045\n",
    "metric name : bbox_head0_loss - value : 0.002939825179055333\n",
    "metric name : bbox_head1_loss - value : 0.007251664996147156\n",
    "metric name : bbox_head2_loss - value : 0.03685268387198448\n",
    "metric name : bbox_head3_loss - value : 0.017508987337350845\n",
    "metric name : bbox_head4_loss - value : 0.029640620574355125\n",
    "metric name : bbox_head5_loss - value : 0.05408470705151558\n",
    "metric name : class_head0_loss - value : 0.0\n",
    "metric name : class_head1_loss - value : 0.0\n",
    "metric name : class_head2_loss - value : 0.0\n",
    "metric name : class_head3_loss - value : 0.0\n",
    "metric name : class_head4_loss - value : 0.0\n",
    "metric name : class_head5_loss - value : 0.0\n",
    "metric name : bbox_head0_custom_iou - value : 0.0\n",
    "metric name : bbox_head0_auc - value : 0.5\n",
    "metric name : bbox_head1_custom_iou - value : 0.0\n",
    "metric name : bbox_head1_auc - value : 0.5\n",
    "metric name : bbox_head2_custom_iou - value : 0.47345197200775146\n",
    "metric name : bbox_head2_auc - value : 0.6789854764938354\n",
    "metric name : bbox_head3_custom_iou - value : 0.0\n",
    "metric name : bbox_head3_auc - value : 0.5\n",
    "metric name : bbox_head4_custom_iou - value : 0.0\n",
    "metric name : bbox_head4_auc - value : 0.5\n",
    "metric name : bbox_head5_custom_iou - value : 0.4330933392047882\n",
    "metric name : bbox_head5_auc - value : 0.7748516201972961\n",
    "metric name : class_head0_custom_iou - value : 1.0\n",
    "metric name : class_head0_auc - value : 0.0\n",
    "metric name : class_head1_custom_iou - value : 0.23481781780719757\n",
    "metric name : class_head1_auc - value : 0.5\n",
    "metric name : class_head2_custom_iou - value : 1.0\n",
    "metric name : class_head2_auc - value : 0.0\n",
    "metric name : class_head3_custom_iou - value : 0.29149797558784485\n",
    "metric name : class_head3_auc - value : 0.5\n",
    "metric name : class_head4_custom_iou - value : 0.9959514141082764\n",
    "metric name : class_head4_auc - value : 0.5\n",
    "metric name : class_head5_custom_iou - value : 1.0\n",
    "metric name : class_head5_auc - value : 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metricas usando linear para bbox y softmax para clases usando bbox normalizadas \n",
    "\n",
    "metric name : loss - value : 0.17858585715293884\n",
    "metric name : bbox_head0_loss - value : 0.0026182420551776886\n",
    "metric name : bbox_head1_loss - value : 0.007999297231435776\n",
    "metric name : bbox_head2_loss - value : 0.07438085228204727\n",
    "metric name : bbox_head3_loss - value : 0.014708954840898514\n",
    "metric name : bbox_head4_loss - value : 0.022220784798264503\n",
    "metric name : bbox_head5_loss - value : 0.05665772780776024\n",
    "metric name : class_head0_loss - value : 0.0\n",
    "metric name : class_head1_loss - value : 0.0\n",
    "metric name : class_head2_loss - value : 0.0\n",
    "metric name : class_head3_loss - value : 0.0\n",
    "metric name : class_head4_loss - value : 0.0\n",
    "metric name : class_head5_loss - value : 0.0\n",
    "metric name : bbox_head0_custom_iou - value : 0.11331154406070709\n",
    "metric name : bbox_head0_auc - value : 0.6937500238418579\n",
    "metric name : bbox_head1_custom_iou - value : 0.09039562940597534\n",
    "metric name : bbox_head1_auc - value : 0.9382540583610535\n",
    "metric name : bbox_head2_custom_iou - value : 0.0823870450258255\n",
    "metric name : bbox_head2_auc - value : 0.8888224959373474\n",
    "metric name : bbox_head3_custom_iou - value : 0.07096018642187119\n",
    "metric name : bbox_head3_auc - value : 0.8575149774551392\n",
    "metric name : bbox_head4_custom_iou - value : 0.19312578439712524\n",
    "metric name : bbox_head4_auc - value : 1.0\n",
    "metric name : bbox_head5_custom_iou - value : 0.07338213920593262\n",
    "metric name : bbox_head5_auc - value : 0.8548846244812012\n",
    "metric name : class_head0_custom_iou - value : 1.0\n",
    "metric name : class_head0_auc - value : 0.0\n",
    "metric name : class_head1_custom_iou - value : 0.2955465614795685\n",
    "metric name : class_head1_auc - value : 0.5\n",
    "metric name : class_head2_custom_iou - value : 1.0\n",
    "metric name : class_head2_auc - value : 0.0\n",
    "metric name : class_head3_custom_iou - value : 0.26720649003982544\n",
    "metric name : class_head3_auc - value : 0.5\n",
    "metric name : class_head4_custom_iou - value : 1.0\n",
    "metric name : class_head4_auc - value : 0.0\n",
    "metric name : class_head5_custom_iou - value : 1.0\n",
    "metric name : class_head5_auc - value : 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metricas usando linear para bbox y softmax para las clases\n",
    "\n",
    "metric name : loss - value : 25366.60546875\n",
    "metric name : bbox_head0_loss - value : 359.25372314453125\n",
    "metric name : bbox_head1_loss - value : 1561.6563720703125\n",
    "metric name : bbox_head2_loss - value : 10720.51953125\n",
    "metric name : bbox_head3_loss - value : 3087.791748046875\n",
    "metric name : bbox_head4_loss - value : 2807.509033203125\n",
    "metric name : bbox_head5_loss - value : 6829.875\n",
    "metric name : class_head0_loss - value : 0.0\n",
    "metric name : class_head1_loss - value : 0.0\n",
    "metric name : class_head2_loss - value : 0.0\n",
    "metric name : class_head3_loss - value : 0.0\n",
    "metric name : class_head4_loss - value : 0.0\n",
    "metric name : class_head5_loss - value : 0.0\n",
    "metric name : bbox_head0_custom_iou - value : 0.017898140475153923\n",
    "metric name : bbox_head0_auc - value : 0.8218186497688293\n",
    "metric name : bbox_head1_custom_iou - value : 0.014815075322985649\n",
    "metric name : bbox_head1_auc - value : 0.8571521043777466\n",
    "metric name : bbox_head2_custom_iou - value : 0.17787572741508484\n",
    "metric name : bbox_head2_auc - value : 0.6871663331985474\n",
    "metric name : bbox_head3_custom_iou - value : 0.05940670892596245\n",
    "metric name : bbox_head3_auc - value : 0.6853057742118835\n",
    "metric name : bbox_head4_custom_iou - value : 0.1887882947921753\n",
    "metric name : bbox_head4_auc - value : 0.8044888377189636\n",
    "metric name : bbox_head5_custom_iou - value : 0.17907769978046417\n",
    "metric name : bbox_head5_auc - value : 0.6605395674705505\n",
    "metric name : class_head0_custom_iou - value : 1.0\n",
    "metric name : class_head0_auc - value : 0.0\n",
    "metric name : class_head1_custom_iou - value : 0.2753036320209503\n",
    "metric name : class_head1_auc - value : 0.5\n",
    "metric name : class_head2_custom_iou - value : 1.0\n",
    "metric name : class_head2_auc - value : 0.0\n",
    "metric name : class_head3_custom_iou - value : 0.31578946113586426\n",
    "metric name : class_head3_auc - value : 0.5\n",
    "metric name : class_head4_custom_iou - value : 0.9919028282165527\n",
    "metric name : class_head4_auc - value : 0.5\n",
    "metric name : class_head5_custom_iou - value : 1.0\n",
    "metric name : class_head5_auc - value : 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metricas con funcion de activacion relu para bbox y sigmoide para clases\n",
    "\n",
    "metric name : loss - value : 34790.7421875\n",
    "metric name : bbox_head0_loss - value : 305.9351806640625\n",
    "metric name : bbox_head1_loss - value : 2154.948486328125\n",
    "metric name : bbox_head2_loss - value : 14255.30078125\n",
    "metric name : bbox_head3_loss - value : 4846.25048828125\n",
    "metric name : bbox_head4_loss - value : 3412.474365234375\n",
    "metric name : bbox_head5_loss - value : 9815.8359375\n",
    "metric name : class_head0_loss - value : 0.0\n",
    "metric name : class_head1_loss - value : 0.0\n",
    "metric name : class_head2_loss - value : 0.0\n",
    "metric name : class_head3_loss - value : 0.0\n",
    "metric name : class_head4_loss - value : 0.0\n",
    "metric name : class_head5_loss - value : 0.0\n",
    "metric name : bbox_head0_custom_iou - value : 0.5003455281257629\n",
    "metric name : bbox_head0_auc - value : 0.8652960062026978\n",
    "metric name : bbox_head1_custom_iou - value : 0.05979517847299576\n",
    "metric name : bbox_head1_auc - value : 0.760465145111084\n",
    "metric name : bbox_head2_custom_iou - value : 0.2548713982105255\n",
    "metric name : bbox_head2_auc - value : 0.7495487928390503\n",
    "metric name : bbox_head3_custom_iou - value : 0.0863441526889801\n",
    "metric name : bbox_head3_auc - value : 0.688020646572113\n",
    "metric name : bbox_head4_custom_iou - value : 0.5988608598709106\n",
    "metric name : bbox_head4_auc - value : 0.8660455942153931\n",
    "metric name : bbox_head5_custom_iou - value : 0.3285883665084839\n",
    "metric name : bbox_head5_auc - value : 0.7502604126930237\n",
    "metric name : class_head0_custom_iou - value : 0.0017493302002549171\n",
    "metric name : class_head0_auc - value : 0.0\n",
    "metric name : class_head1_custom_iou - value : 0.0\n",
    "metric name : class_head1_auc - value : 0.5\n",
    "metric name : class_head2_custom_iou - value : 0.0\n",
    "metric name : class_head2_auc - value : 0.0\n",
    "metric name : class_head3_custom_iou - value : 0.0\n",
    "metric name : class_head3_auc - value : 0.5\n",
    "metric name : class_head4_custom_iou - value : 0.0\n",
    "metric name : class_head4_auc - value : 0.5\n",
    "metric name : class_head5_custom_iou - value : 0.0\n",
    "metric name : class_head5_auc - value : 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrica con funciones de activacion sigmoide para ambos bbox y clases\n",
    "\n",
    "metric name : loss - value : 62710.8125\n",
    "metric name : bbox_head0_loss - value : 579.985107421875\n",
    "metric name : bbox_head1_loss - value : 2669.833984375\n",
    "metric name : bbox_head2_loss - value : 27072.76171875\n",
    "metric name : bbox_head3_loss - value : 5961.2978515625\n",
    "metric name : bbox_head4_loss - value : 7870.39404296875\n",
    "metric name : bbox_head5_loss - value : 18556.5390625\n",
    "metric name : class_head0_loss - value : 0.0\n",
    "metric name : class_head1_loss - value : 0.0\n",
    "metric name : class_head2_loss - value : 0.0\n",
    "metric name : class_head3_loss - value : 0.0\n",
    "metric name : class_head4_loss - value : 0.0\n",
    "metric name : class_head5_loss - value : 0.0\n",
    "metric name : bbox_head0_custom_iou - value : 0.4661273956298828\n",
    "metric name : bbox_head0_auc - value : 0.9547063112258911\n",
    "metric name : bbox_head1_custom_iou - value : 0.1277552843093872\n",
    "metric name : bbox_head1_auc - value : 0.9228104948997498\n",
    "metric name : bbox_head2_custom_iou - value : 0.29493966698646545\n",
    "metric name : bbox_head2_auc - value : 0.844988226890564\n",
    "metric name : bbox_head3_custom_iou - value : 0.06415440887212753\n",
    "metric name : bbox_head3_auc - value : 0.7879701852798462\n",
    "metric name : bbox_head4_custom_iou - value : 0.3272666037082672\n",
    "metric name : bbox_head4_auc - value : 0.9145087599754333\n",
    "metric name : bbox_head5_custom_iou - value : 0.2419300675392151\n",
    "metric name : bbox_head5_auc - value : 0.796000599861145\n",
    "metric name : class_head0_custom_iou - value : 1.0\n",
    "metric name : class_head0_auc - value : 0.0\n",
    "metric name : class_head1_custom_iou - value : 0.0\n",
    "metric name : class_head1_auc - value : 0.5\n",
    "metric name : class_head2_custom_iou - value : 0.9959514141082764\n",
    "metric name : class_head2_auc - value : 0.5\n",
    "metric name : class_head3_custom_iou - value : 0.0\n",
    "metric name : class_head3_auc - value : 0.5\n",
    "metric name : class_head4_custom_iou - value : 0.0\n",
    "metric name : class_head4_auc - value : 0.5\n",
    "metric name : class_head5_custom_iou - value : 0.0\n",
    "metric name : class_head5_auc - value : 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision de predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array \n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SHAPE = (500, 500)\n",
    "\n",
    "image_folder_melu = '/Volumes/ADATA HD680/Shared/Files From d.localized/Maestria/tesis/MELU_IMAGES'\n",
    "image_folder_unal = os.path.join(os.pardir, 'data_UN', 'imagenes')\n",
    "\n",
    "path_single_image_to_predict_unal = os.path.join(image_folder_unal,'COL000000671.png')\n",
    "path_single_image_to_predict_melu = os.path.join(image_folder_melu, 'MELUA000673a_sp63527816422508798934_medium.png')\n",
    "\n",
    "predict_image_1 = load_img(path_single_image_to_predict_unal)\n",
    "imaga_re_size_predict_1 = tf.image.resize(predict_image_1, IMG_SHAPE)\n",
    "image_array_predict_1 = img_to_array(imaga_re_size_predict_1)\n",
    "\n",
    "image_array_predict_1 = tf.expand_dims(image_array_predict_1, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 402ms/step\n",
      "8/8 [==============================] - 10s 964ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflowLocal.utils_tensorflow import get_maximun_number_of_annotation_in_set, train_batch_consolidation, read_data, image_test_set, image_train_set,dump_file\n",
    "import pickle\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath( os.pardir)))\n",
    "from tensorflowLocal.models import CustomIoUMetric\n",
    "from utils.model_evaluator import TensorflowEvaluator,PytorchEvaluator\n",
    "\n",
    "from utils.paths import get_project_models,get_project_annotations,get_project_configs\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath( os.pardir)))\n",
    "\n",
    "# pytorchmodelCNNMelu = get_project_models('pytorch/CNN/5_10_0.001_MELU_model_CNN_pytorch.pth')\n",
    "pytorchmodelCNNUN = get_project_models('pytorch/CNN/50_10_0.001_UN_model_CNN_pytorch.pt')\n",
    "config = get_project_configs('json/torch_simple.json')\n",
    "SETS_PATHS = get_project_configs('sets')\n",
    "# configUn = get_project_configs('json/torch_simple.json')\n",
    "COCO_ANNOTATION_FILEUn = get_project_annotations('dataset.json')\n",
    "COCO_ANNOTATION_FILEMelu = get_project_annotations('dataset_melu.json')\n",
    "# meluModelPytorchWeigths = torch.load(pytorchmodelCNNMelu)\n",
    "UnModelPytorchWeigths = torch.load(pytorchmodelCNNUN)\n",
    "\n",
    "\n",
    "tensorflowmodelCNNMelu = get_project_models('TensorFlow/CNN/5_10_MELU_model_CNN_tensorflow.keras')\n",
    "tensorflowmodelCNNUN = get_project_models('TensorFlow/CNN/5_10_UN_model_CNN_tensorflow.keras')\n",
    "\n",
    "\n",
    "with custom_object_scope({'CustomIoUMetric': CustomIoUMetric}):\n",
    "    unModelTensorflow = tf.keras.models.load_model(tensorflowmodelCNNUN) \n",
    "    meluModelTensorflow = tf.keras.models.load_model(tensorflowmodelCNNMelu)\n",
    "\n",
    "IMG_SHAPE = (500, 500)\n",
    "total_test_images = []\n",
    "total_test_targets = []\n",
    "\n",
    "images,annotations,categorias,_ = read_data(COCO_ANNOTATION_FILEUn)\n",
    "\n",
    "# train_images = image_train_set(images,0.75)\n",
    "# dump_file(train_images,os.path.join(SETS_PATHS,'train_image_set'))\n",
    "# test_images = image_test_set(images,train_images)\n",
    "# dump_file(test_images,os.path.join(SETS_PATHS,'test_image_set'))\n",
    "\n",
    "test_bites = open(get_project_configs('sets/test_image_set'),'rb')\n",
    "test_images = pickle.load(test_bites)#image_test_set(images,train_images)\n",
    "\n",
    "max_n_boxes_test = get_maximun_number_of_annotation_in_set(annotations,images)\n",
    "images_for_test , test_targets = train_batch_consolidation(test_images,total_test_images,annotations,total_test_targets,max_n_boxes_test,IMG_SHAPE)\n",
    "images_for_test = tf.cast(images_for_test, dtype=tf.float32)\n",
    "\n",
    "\n",
    "evaluateModelMeluTensorflowUN = TensorflowEvaluator(model = unModelTensorflow,\n",
    "                                                  data=images_for_test,\n",
    "                                                  device='mps',\n",
    "                                                  targets = test_targets,)\n",
    "\n",
    "evaluateModelMeluTensorflowMelu = TensorflowEvaluator(model = meluModelTensorflow,\n",
    "                                                  data=images_for_test,\n",
    "                                                  device='mps',\n",
    "                                                  targets = test_targets,)\n",
    "\n",
    "\n",
    "outputsUN = evaluateModelMeluTensorflowUN.predict()\n",
    "outputsMElu = evaluateModelMeluTensorflowMelu.predict()\n",
    "\n",
    "\n",
    "# evaluations = scanned_model.evaluate(images_for_test,test_targets,batch_size=len(images_for_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('targets.txt','w') as file:\n",
    "    file.write(str(test_targets))\n",
    "# json.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets_order = dict(sorted(test_targets.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [test_targets[x] for x in test_targets_order.keys() if x.startswith('class_head')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.paths import get_project_models\n",
    "import json \n",
    "\n",
    "metrics_file_path = get_project_models('TensorFlow/CNN/5_1_UN_model_CNN_tensorflow_results.json')\n",
    "\n",
    "with open(metrics_file_path,'+r') as metrics_files :\n",
    "    metrics_dict = json.load(metrics_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "presicion_class_results = [metrics_dict[x] for x in metrics_dict.keys() if x.startswith('class') and x.endswith('precision') ]\n",
    "recall_class_results = [metrics_dict[x] for x in metrics_dict.keys() if x.startswith('class') and x.endswith('recall') ]\n",
    "accuracy_call_results = [metrics_dict[x] for x in metrics_dict.keys() if x.startswith('class') and x.endswith('accuracy') ]\n",
    "custom_iou_call_results = [metrics_dict[x] for x in metrics_dict.keys() if x.startswith('class') and x.endswith('custom_iou') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def graphMetricHistory(history_epoch:list,metric:str,legend:list[dict])->None:#values_x:list,values_y:list\n",
    "    fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n",
    "    for i,label in enumerate(history_epoch):\n",
    "        print(label,i)\n",
    "        print([x['name'] for x in legend if x['id'] == i])\n",
    "        ax.plot(label[0], label[1],label = [x['name'] for x in legend if x['id'] == i] )\n",
    "    ax.set_title(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert categorias[0]['id'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_draw = zip(accuracy_call_results,[range(len(accuracy_call_results[0])) for x in range(len(accuracy_call_results))])##,categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.046296294778585434, 0.08197832107543945, 0.1334688365459442, 0.13437217473983765, 0.15695573389530182], range(0, 5)) 0\n",
      "([0.0614272803068161, 0.08514001965522766, 0.13233965635299683, 0.1334688365459442, 0.1560523957014084], range(0, 5)) 1\n",
      "([0.03477868065237999, 0.07271905988454819, 0.13685636222362518, 0.13911472260951996, 0.15785907208919525], range(0, 5)) 2\n",
      "([0.031165311112999916, 0.09214092046022415, 0.1384372115135193, 0.13753387331962585, 0.15740740299224854], range(0, 5)) 3\n",
      "([0.03071364015340805, 0.03703703731298447, 0.08988256752490997, 0.09914182126522064, 0.13753387331962585], range(0, 5)) 4\n",
      "([0.044263776391744614, 0.05352303385734558, 0.10298103094100952, 0.1036585345864296, 0.14024390280246735], range(0, 5)) 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x270 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphMetricHistory(to_draw,'accuracy',categorias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict['class_head5_precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpuTclassesUN = outputsUN[6:]\n",
    "print(outpuTclassesUN[5][0][-1])\n",
    "print(target_classes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "totalCMperImage = []\n",
    "\n",
    "for category in tqdm(range(len(target_classes)), total= len(target_classes)):\n",
    "   for vecInd in range(len(target_classes[category])):   \n",
    "      targetSinglePrediction = target_classes[category][0][0]\n",
    "      outputSinglePrediction = outpuTclassesUN[category][0][0]\n",
    "\n",
    "      cm_per_individual_prediction = tf.math.confusion_matrix(\n",
    "         outputSinglePrediction,\n",
    "         targetSinglePrediction,\n",
    "         num_classes=None,\n",
    "         weights=None,\n",
    "         dtype=tf.dtypes.int32,\n",
    "         name=None\n",
    "      )\n",
    "\n",
    "      totalCMperImage.append(cm_per_individual_prediction.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCMperImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cm in totalCMperImage: #[0][0]#.argsort()\n",
    "    isEqual = (totalCMperImage[0] == cm).all() \n",
    "    assert isEqual == 1\n",
    "    #print(isEqual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputsUN[6:]))\n",
    "print('---------------------------------')\n",
    "print(len(outputsMElu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModelMeluTensorflow.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilePath50UN = get_project_models('pytorch/CNN/50_10_0.001_UN_model_CNN_pytorch_results.json')\n",
    "outputFilePath5UN = get_project_models('pytorch/CNN/5_10_0.001_UN_model_CNN_pytorch_results.json')\n",
    "outputFilePath5Melu = get_project_models('pytorch/CNN/5_10_0.001_MELU_model_CNN_pytorch_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import loads\n",
    "import json\n",
    "from typing import Union\n",
    "cnf_melu = json.load(open(COCO_ANNOTATION_FILEMelu))\n",
    "cngf_un = json.load(open(COCO_ANNOTATION_FILEUn))\n",
    "num_clases_melu = len(cnf_melu['categories'])\n",
    "num_clases_un = len(cngf_un['categories'])\n",
    "print(num_clases_melu,num_clases_un)\n",
    "\n",
    "def load_model_history(outputFilePath:str)->dict:\n",
    "    with open(outputFilePath,'rb') as file:\n",
    "        data = file.read()\n",
    "        data_decoded = loads(data)\n",
    "        return data_decoded\n",
    "\n",
    "def get_epochs_by_loss_type(data:dict,loss_type:str)-> Union[list, None]:\n",
    "    match loss_type:\n",
    "        case 'bbox':\n",
    "            return data['train_loss_history']['loss_bbox_per_class'] \n",
    "        case 'labels':\n",
    "            return data['train_loss_history']['loss_class_per_class']\n",
    "        case _ :\n",
    "            print('loss type not found choose between bbox or labels')\n",
    "            return None\n",
    "\n",
    "def get_loss_values_per_class(loss_history:dict,num_clases:int)->list:\n",
    "    loss_values = dict(zip([i for i in range(num_clases)],[[] for i in range(num_clases)])) #[]\n",
    "    for epoch in loss_history:\n",
    "        for i,ctg in enumerate(epoch):\n",
    "            loss_values[i].append(ctg.item())\n",
    "    return loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_loss_history50un = get_epochs_by_loss_type(load_model_history(outputFilePath50UN),'bbox') \n",
    "label_loss_history50un = get_epochs_by_loss_type(load_model_history(outputFilePath50UN),'labels')\n",
    "\n",
    "bbox_loss_history5un = get_epochs_by_loss_type(load_model_history(outputFilePath5UN),'bbox') \n",
    "label_loss_history5un = get_epochs_by_loss_type(load_model_history(outputFilePath5UN),'labels')\n",
    "\n",
    "bbox_loss_history5Melu = get_epochs_by_loss_type(load_model_history(outputFilePath5Melu),'bbox') \n",
    "label_loss_history5Melu = get_epochs_by_loss_type(load_model_history(outputFilePath5Melu),'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_loss_per_class50UN = get_loss_values_per_class(bbox_loss_history50un,num_clases_un)\n",
    "labels_loss_per_class50UN = get_loss_values_per_class(label_loss_history50un,num_clases_un)\n",
    "\n",
    "bbox_loss_per_class5UN = get_loss_values_per_class(bbox_loss_history5un,num_clases_un)\n",
    "labels_loss_per_class5UN = get_loss_values_per_class(label_loss_history5un,num_clases_un)\n",
    "\n",
    "bbox_loss_per_class5Melu = get_loss_values_per_class(bbox_loss_history5Melu,num_clases_melu)\n",
    "labels_loss_per_class5Melu = get_loss_values_per_class(label_loss_history5Melu,num_clases_melu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_bbox_loss_per_class50UN = pd.DataFrame(bbox_loss_per_class50UN)\n",
    "df_labels_loss_per_class50UN = pd.DataFrame(labels_loss_per_class50UN)\n",
    "\n",
    "df_bbox_loss_per_class5UN = pd.DataFrame(bbox_loss_per_class5UN)\n",
    "df_labels_loss_per_class5UN = pd.DataFrame(labels_loss_per_class5UN)\n",
    "\n",
    "df_bbox_loss_per_class5MELU = pd.DataFrame(bbox_loss_per_class5Melu)\n",
    "df_labels_loss_per_class5MELU = pd.DataFrame(labels_loss_per_class5Melu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_bbox_loss_per_class50UN.plot(kind='line',figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_loss_per_class50UN.plot(kind='line',figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_loss_per_class5UN.plot(kind='line',figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbox_loss_per_class5UN.plot(kind='line',figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbox_loss_per_class5MELU.plot(kind='line',figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_loss_per_class5MELU.plot(kind='line',figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pytorchLocal/naive_model_torch.py --epochs 50 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading annotations into memory...\n",
    "Done (t=0.28s)\n",
    "creating index...\n",
    "index created!\n",
    "{'train_loss_history': {'average_loss': [4380.3649894026785, 4375.904133711337, 4379.215625166339, 4372.668720603321, 4375.11512504476, 4378.137124466505, 4369.766317122416, 4373.145930465363, 4371.499491531216, 4371.230194592705, 4367.8263472116705, 4372.540797645, 4370.913987344911, 4371.002862168894, 4374.001628612284, 4376.0886001374265, 4370.035842777369, 4374.2901314563815, 4372.42737583835, 4368.3673155055985, 4371.291342859631, 4373.57856103561, 4375.297025839439, 4370.9130210652, 4376.742542861083, 4375.481325321428, 4370.586905972365, 4371.271035788339, 4375.431016096799, 4372.933265910498, 4371.756114176457, 4376.653057345298, 4374.604250541964, 4370.049034234428, 4375.653611254936, 4368.308149473038, 4370.654296421348, 4375.160164340135, 4378.265956317504, 4378.2198634143215, 4369.158970629331, 4373.735988641753, 4374.000485710553, 4374.182400039195, 4367.121199299802, 4373.7123409494525, 4374.3494531673405, 4371.381613799019, 4376.661265430232, 4369.38490618165], 'loss_bbox_per_class': [[0.8104857206344604, 0.8176897764205933, 0.8229198455810547, 0.8096415996551514, 0.8206740021705627, 0.825037956237793, 0.8015214204788208, 0.8146259188652039, 0.814335286617279, 0.8193675875663757, 0.8276734352111816], [0.8353760242462158, 0.8430627584457397, 0.8373390436172485, 0.8351219892501831, 0.829986572265625, 0.841965913772583, 0.8323310017585754, 0.8378605246543884, 0.8371990919113159, 0.8350871801376343, 0.8411180973052979], [0.8482943177223206, 0.8338451385498047, 0.8447428941726685, 0.8401917219161987, 0.8438965082168579, 0.8353345394134521, 0.8363432884216309, 0.8390350341796875, 0.8327634334564209, 0.8329669237136841, 0.8374373912811279], [0.8087658286094666, 0.8011155128479004, 0.8030844926834106, 0.8091509342193604, 0.8075470328330994, 0.7963714599609375, 0.7963510751724243, 0.7984287738800049, 0.8036046028137207, 0.8017353415489197, 0.8106984496116638], [0.8382164239883423, 0.8410639762878418, 0.8466254472732544, 0.8388831615447998, 0.8385398387908936, 0.8414523005485535, 0.8371862173080444, 0.8364005088806152, 0.830680787563324, 0.8319884538650513, 0.8470717072486877], [0.8513493537902832, 0.8476675748825073, 0.8505289554595947, 0.8481378555297852, 0.8569766283035278, 0.8498672246932983, 0.8538275361061096, 0.8531808853149414, 0.8518306016921997, 0.8493341207504272, 0.858860969543457], [0.8581665754318237, 0.847824215888977, 0.8571958541870117, 0.8462755680084229, 0.8587659597396851, 0.8487277626991272, 0.8484922051429749, 0.8550859093666077, 0.8483682870864868, 0.846773624420166, 0.8556438088417053], [0.8158589601516724, 0.8045116662979126, 0.8167284727096558, 0.8062372207641602, 0.8147243857383728, 0.8046833276748657, 0.8046393990516663, 0.8091990351676941, 0.8039305806159973, 0.8039067387580872, 0.808976411819458], [0.8398988246917725, 0.8217719793319702, 0.8334364295005798, 0.8329780697822571, 0.8372834920883179, 0.8351097702980042, 0.8307715654373169, 0.8411937952041626, 0.8373157978057861, 0.8269051909446716, 0.8345478177070618], [0.8197764158248901, 0.8124594688415527, 0.8175638914108276, 0.8095071911811829, 0.8204796314239502, 0.8083755970001221, 0.7980561852455139, 0.817500114440918, 0.8124488592147827, 0.8114912509918213, 0.8202101588249207], [0.851542592048645, 0.8492625951766968, 0.8546873331069946, 0.8455250263214111, 0.8490171432495117, 0.84209144115448, 0.8393737077713013, 0.8500970005989075, 0.8441269993782043, 0.8523097038269043, 0.8460168242454529], [0.8495305776596069, 0.8441809415817261, 0.8554499745368958, 0.8458248972892761, 0.8481518030166626, 0.845376193523407, 0.8471620678901672, 0.8519848585128784, 0.8475204706192017, 0.8454398512840271, 0.8493969440460205], [0.8249258399009705, 0.8192116022109985, 0.8220428824424744, 0.8160701394081116, 0.8232583999633789, 0.818079948425293, 0.8197913765907288, 0.8203809261322021, 0.8134176731109619, 0.8187524080276489, 0.8189198970794678], [0.8475337028503418, 0.8407350182533264, 0.8484679460525513, 0.8328876495361328, 0.8432455062866211, 0.8407791256904602, 0.8410337567329407, 0.850574254989624, 0.8368074893951416, 0.8420518636703491, 0.836711049079895], [0.8410803079605103, 0.8460208773612976, 0.8441693782806396, 0.8449281454086304, 0.8529511094093323, 0.8461321592330933, 0.8473318219184875, 0.8478687405586243, 0.8403993248939514, 0.8390956521034241, 0.8500120043754578], [0.8520063161849976, 0.8521465063095093, 0.8589829802513123, 0.8505488634109497, 0.8563200235366821, 0.8635969161987305, 0.8491350412368774, 0.8553021550178528, 0.856214702129364, 0.8575589656829834, 0.856757402420044], [0.8027572631835938, 0.804785966873169, 0.8080282211303711, 0.7987223863601685, 0.8055517673492432, 0.8035842776298523, 0.80265212059021, 0.8062953948974609, 0.790989875793457, 0.7994164228439331, 0.7993230819702148], [0.8404339551925659, 0.8356824517250061, 0.8387669920921326, 0.8308569192886353, 0.8335821628570557, 0.8317012190818787, 0.8391827940940857, 0.833744466304779, 0.8316875696182251, 0.8358703255653381, 0.8374306559562683], [0.829123854637146, 0.8214513659477234, 0.8343546390533447, 0.8285090923309326, 0.840582013130188, 0.8406669497489929, 0.8293924331665039, 0.8257449269294739, 0.8285908699035645, 0.830524206161499, 0.8282831907272339], [0.8505744934082031, 0.8525829315185547, 0.8629812002182007, 0.8459197282791138, 0.8545175790786743, 0.858917236328125, 0.8476826548576355, 0.8570209741592407, 0.8526861071586609, 0.8571265935897827, 0.8632482886314392], [0.8180005550384521, 0.8187676668167114, 0.819932222366333, 0.820026695728302, 0.8171312808990479, 0.8324354887008667, 0.8175637125968933, 0.8181445002555847, 0.8158829212188721, 0.822756826877594, 0.8140448927879333], [0.8363139629364014, 0.8443522453308105, 0.8438735604286194, 0.8388122916221619, 0.8427180647850037, 0.8463072776794434, 0.8348992466926575, 0.8464717864990234, 0.8366782665252686, 0.8398382067680359, 0.839083731174469], [0.8343680500984192, 0.8353472948074341, 0.8431056141853333, 0.8297823667526245, 0.8504998683929443, 0.842188835144043, 0.8310980200767517, 0.8403815031051636, 0.829751193523407, 0.841740071773529, 0.8397467732429504], [0.831915020942688, 0.8276395797729492, 0.839881956577301, 0.8244684934616089, 0.8331854939460754, 0.8411264419555664, 0.825708270072937, 0.8311340808868408, 0.8330665826797485, 0.8246527314186096, 0.815506100654602], [0.8950139880180359, 0.8913998603820801, 0.8954879641532898, 0.8953858613967896, 0.8958063125610352, 0.8893579244613647, 0.8907605409622192, 0.8885102868080139, 0.884232759475708, 0.8859226703643799, 0.8788555860519409], [0.8442966938018799, 0.8408275842666626, 0.8433288931846619, 0.8265945911407471, 0.8440992832183838, 0.8401262760162354, 0.8338800668716431, 0.8331592679023743, 0.8387783169746399, 0.8428949117660522, 0.8299200534820557], [0.8370748162269592, 0.843636691570282, 0.8491138219833374, 0.8422881364822388, 0.8432629704475403, 0.844947099685669, 0.8417657613754272, 0.8411775231361389, 0.8355794548988342, 0.8393115997314453, 0.8324673771858215], [0.8246623873710632, 0.831744909286499, 0.8396921753883362, 0.8181716203689575, 0.832303524017334, 0.8351352214813232, 0.8277696967124939, 0.8334881067276001, 0.8298885822296143, 0.8386390805244446, 0.8276903629302979], [0.8072746396064758, 0.8048509359359741, 0.8119155764579773, 0.8145132660865784, 0.812798798084259, 0.8114500045776367, 0.8023544549942017, 0.810194194316864, 0.807717502117157, 0.8101968169212341, 0.8003864884376526], [0.8354254961013794, 0.8360644578933716, 0.837030291557312, 0.8303164839744568, 0.8411164283752441, 0.8403168320655823, 0.8377931118011475, 0.8338482975959778, 0.8340975046157837, 0.8428303003311157, 0.8366202116012573], [0.8285398483276367, 0.8275608420372009, 0.8259576559066772, 0.814696192741394, 0.8294295072555542, 0.820123553276062, 0.8224793672561646, 0.8181549310684204, 0.8294835090637207, 0.826555609703064, 0.8221426010131836], [0.8523931503295898, 0.8408055305480957, 0.8395171165466309, 0.8390681147575378, 0.8401707410812378, 0.8397706151008606, 0.840734601020813, 0.8402183651924133, 0.8436065912246704, 0.8310831785202026, 0.8359980583190918], [0.7984018921852112, 0.7990107536315918, 0.8026929497718811, 0.8000755310058594, 0.8092653751373291, 0.8009440898895264, 0.7887899875640869, 0.8003305196762085, 0.7979097962379456, 0.7892823815345764, 0.7922052145004272], [0.8182491660118103, 0.8260821104049683, 0.8206574320793152, 0.8207771182060242, 0.8167966604232788, 0.8292116522789001, 0.8188767433166504, 0.8200011849403381, 0.81209796667099, 0.8219290971755981, 0.8239967823028564], [0.8142717480659485, 0.8115950226783752, 0.817908525466919, 0.808277428150177, 0.812272846698761, 0.8078356981277466, 0.8061760663986206, 0.8126484155654907, 0.8058738708496094, 0.8023288249969482, 0.8037792444229126], [0.7602779865264893, 0.7648134827613831, 0.7655616402626038, 0.76156085729599, 0.7740099430084229, 0.7835996150970459, 0.7491130828857422, 0.7571296691894531, 0.7730002999305725, 0.7760459184646606, 0.7548045516014099], [0.8189441561698914, 0.8288527131080627, 0.8377106189727783, 0.8103446364402771, 0.8295304775238037, 0.8323719501495361, 0.8205074667930603, 0.819267988204956, 0.8417900204658508, 0.8308718800544739, 0.8171238303184509], [0.8292331695556641, 0.8303640484809875, 0.8354219794273376, 0.8286570906639099, 0.834850549697876, 0.8365678191184998, 0.824640154838562, 0.8294284343719482, 0.8276974558830261, 0.8294292092323303, 0.821828305721283], [0.8139339685440063, 0.8165262937545776, 0.813606858253479, 0.8123147487640381, 0.8171061873435974, 0.8144288659095764, 0.8097512125968933, 0.8119232058525085, 0.811072826385498, 0.8045891523361206, 0.806920051574707], [0.8224068284034729, 0.8296788930892944, 0.832382082939148, 0.8171144723892212, 0.8303331136703491, 0.829369843006134, 0.8258737325668335, 0.83567214012146, 0.8347808718681335, 0.8233938217163086, 0.8378502130508423], [0.8400830030441284, 0.845485508441925, 0.842668890953064, 0.8395611643791199, 0.8520972728729248, 0.8464238047599792, 0.8333205580711365, 0.8407686948776245, 0.8421752452850342, 0.8431591987609863, 0.8413742184638977], [0.8395295739173889, 0.832086980342865, 0.8344916105270386, 0.83897864818573, 0.8377870321273804, 0.8396557569503784, 0.8272542357444763, 0.8260676860809326, 0.8297354578971863, 0.8345685005187988, 0.8296579718589783], [0.8121590614318848, 0.8151767253875732, 0.8216387033462524, 0.8119143843650818, 0.8227814435958862, 0.8114234209060669, 0.8169676661491394, 0.8071831464767456, 0.8113368153572083, 0.82179856300354, 0.8071764707565308], [0.8364903926849365, 0.8315072059631348, 0.8386073708534241, 0.8305654525756836, 0.8381679654121399, 0.8438040018081665, 0.8277571797370911, 0.8276044726371765, 0.8326852321624756, 0.8386331796646118, 0.8333745002746582], [0.817835807800293, 0.8239182233810425, 0.8335962891578674, 0.8275978565216064, 0.824729323387146, 0.8367201089859009, 0.8187075257301331, 0.8108978271484375, 0.826566219329834, 0.8247970342636108, 0.8161395192146301], [0.8175446391105652, 0.80992192029953, 0.8227940797805786, 0.814923882484436, 0.8179996013641357, 0.8117584586143494, 0.8078319430351257, 0.806694507598877, 0.8141087889671326, 0.8136643767356873, 0.8093714714050293], [0.8483161330223083, 0.8478689193725586, 0.8554049730300903, 0.8451380133628845, 0.8488035798072815, 0.846015453338623, 0.8466733694076538, 0.8481132984161377, 0.8500833511352539, 0.8542188405990601, 0.8561956882476807], [0.8426157832145691, 0.8561996817588806, 0.8525025844573975, 0.8522855043411255, 0.8590424656867981, 0.8598954677581787, 0.852103590965271, 0.849150538444519, 0.8531285524368286, 0.8476623296737671, 0.8556686639785767], [0.8731178045272827, 0.8741878867149353, 0.8813182711601257, 0.8750730752944946, 0.8815889954566956, 0.8734418153762817, 0.8766710758209229, 0.8765878081321716, 0.8688420057296753, 0.875951886177063, 0.8683915138244629], [0.8408553600311279, 0.8365187644958496, 0.8385416269302368, 0.8443046808242798, 0.839846134185791, 0.8490718603134155, 0.8398480415344238, 0.8388376832008362, 0.8447892665863037, 0.8360679745674133, 0.8449455499649048]], 'loss_class_per_class': [[400.31634521484375, 400.6247253417969, 401.0262451171875, 401.20074462890625, 400.1377258300781, 400.1221618652344, 400.4742126464844, 400.5751953125, 400.4871826171875, 401.07275390625, 400.3517150878906], [432.2643127441406, 432.4538269042969, 431.853515625, 433.93084716796875, 431.8068542480469, 432.208740234375, 431.7162780761719, 431.5884704589844, 431.7162780761719, 431.6368713378906, 430.7168273925781], [342.1110534667969, 342.3186340332031, 342.72955322265625, 341.6351623535156, 341.86553955078125, 342.4112548828125, 342.3486328125, 343.09429931640625, 342.19049072265625, 341.7667236328125, 342.2174072265625], [420.24554443359375, 421.50567626953125, 420.07574462890625, 420.7491455078125, 420.9074401855469, 419.7897033691406, 419.6653137207031, 420.0827941894531, 419.5451965332031, 419.40228271484375, 419.7053527832031], [323.6232604980469, 324.77972412109375, 325.64007568359375, 325.9510803222656, 325.227783203125, 325.4776916503906, 323.699462890625, 324.67889404296875, 325.6548767089844, 325.6737060546875, 325.2747802734375], [306.5662536621094, 306.398681640625, 306.079833984375, 306.8674011230469, 306.2529296875, 306.0677795410156, 306.6631774902344, 306.36212158203125, 305.8446350097656, 306.43548583984375, 305.882080078125], [300.2950439453125, 300.414794921875, 300.1826171875, 300.6311340332031, 299.8495178222656, 299.3133239746094, 300.47845458984375, 300.49786376953125, 299.90264892578125, 299.41943359375, 299.8995056152344], [405.4970397949219, 406.9920654296875, 406.10205078125, 406.0452880859375, 406.59991455078125, 405.46319580078125, 405.8417053222656, 406.0690002441406, 406.6377258300781, 406.5219421386719, 405.7386779785156], [339.1918029785156, 338.20281982421875, 338.6712951660156, 339.2707824707031, 338.1162109375, 339.25579833984375, 338.38885498046875, 337.99517822265625, 338.833251953125, 338.67755126953125, 338.2656555175781], [403.5252380371094, 403.298583984375, 403.1716003417969, 403.3490295410156, 402.98956298828125, 403.5670471191406, 403.12811279296875, 403.0419006347656, 404.02593994140625, 402.94140625, 403.1153259277344], [327.3114013671875, 327.24517822265625, 327.794921875, 327.60736083984375, 327.517822265625, 327.7666320800781, 327.30865478515625, 327.56695556640625, 327.3344421386719, 327.6517333984375, 327.73089599609375], [328.1443176269531, 328.134033203125, 328.06500244140625, 327.42578125, 328.1240234375, 328.61260986328125, 328.3091735839844, 328.2366943359375, 329.0064392089844, 328.046142578125, 328.33447265625], [387.9501037597656, 387.88201904296875, 387.895751953125, 389.09539794921875, 388.0424499511719, 388.25335693359375, 387.9866027832031, 387.9458923339844, 387.9276428222656, 388.1448974609375, 388.1126708984375], [335.6019592285156, 335.3193664550781, 335.5296630859375, 334.3454284667969, 335.3804626464844, 335.2923889160156, 335.8649597167969, 335.3218078613281, 335.341552734375, 335.3240966796875, 335.3182067871094], [325.6493835449219, 326.2183837890625, 325.7282409667969, 325.0439147949219, 325.64923095703125, 325.7848815917969, 325.8292236328125, 325.8354187011719, 325.4003601074219, 325.8357849121094, 325.6889343261719], [286.672119140625, 286.7764587402344, 286.8660888671875, 286.88983154296875, 286.3782043457031, 286.654296875, 286.58941650390625, 286.43402099609375, 286.7763977050781, 286.6326599121094, 286.31524658203125], [412.8141784667969, 412.6037292480469, 412.5126953125, 414.31439208984375, 412.52447509765625, 412.3558349609375, 412.2483215332031, 412.4378662109375, 412.88482666015625, 412.4956970214844, 412.3826599121094], [339.364501953125, 339.4407653808594, 339.33905029296875, 340.4500427246094, 339.1542053222656, 339.4217224121094, 339.752685546875, 339.5514831542969, 339.3541259765625, 339.4630432128906, 339.73321533203125], [358.48663330078125, 358.48663330078125, 358.42779541015625, 357.6452941894531, 358.4548645019531, 358.2696228027344, 358.48663330078125, 358.3948059082031, 358.9540710449219, 358.3127136230469, 358.21966552734375], [292.5924072265625, 292.54248046875, 292.5843811035156, 294.405029296875, 292.50457763671875, 292.5997314453125, 292.5315856933594, 292.6673583984375, 292.5032043457031, 292.7228698730469, 292.4739685058594], [379.21014404296875, 379.1656188964844, 379.2200927734375, 379.2675476074219, 379.12274169921875, 379.2064514160156, 379.3503112792969, 379.1888732910156, 379.1983947753906, 379.07354736328125, 379.1572570800781], [319.2860107421875, 319.15179443359375, 319.2995910644531, 319.35394287109375, 319.3476257324219, 319.4900817871094, 319.14837646484375, 319.528076171875, 319.2687072753906, 319.3787841796875, 319.28961181640625], [326.23980712890625, 325.9423522949219, 325.9779052734375, 325.9671630859375, 326.0283508300781, 326.0469665527344, 326.0008544921875, 326.00103759765625, 326.3808288574219, 326.0033264160156, 325.8796081542969], [362.6781311035156, 362.54669189453125, 362.3238830566406, 362.30792236328125, 362.41558837890625, 362.4815368652344, 362.29376220703125, 362.180908203125, 362.29498291015625, 362.2146301269531, 362.35089111328125], [463.6192932128906, 463.6248779296875, 463.588134765625, 463.57574462890625, 463.5928955078125, 463.58538818359375, 463.6123962402344, 463.7621765136719, 463.9630432128906, 463.63031005859375, 463.4833068847656], [337.7718200683594, 337.89892578125, 337.6488952636719, 337.6590576171875, 337.6808776855469, 337.701171875, 337.66259765625, 337.5974426269531, 337.70526123046875, 337.6608581542969, 337.9134521484375], [324.7081604003906, 324.7490234375, 324.8575134277344, 325.01690673828125, 324.8862609863281, 324.8270568847656, 324.8583984375, 324.6762390136719, 324.8439636230469, 324.9217834472656, 325.01885986328125], [358.5135498046875, 358.63580322265625, 358.56976318359375, 358.4934387207031, 358.425048828125, 358.458251953125, 358.486572265625, 358.4720458984375, 358.7264709472656, 358.49566650390625, 358.44110107421875], [405.9556579589844, 406.0280456542969, 405.9302673339844, 405.9427795410156, 405.8958435058594, 405.93743896484375, 405.8793029785156, 405.8464660644531, 405.8440856933594, 405.8645324707031, 405.87066650390625], [564.9044799804688, 564.908447265625, 565.0196533203125, 564.8903198242188, 565.0377197265625, 565.0806274414062, 565.0735473632812, 565.0377807617188, 564.9158935546875, 565.0368041992188, 565.006103515625], [358.8656311035156, 358.8141784667969, 358.8801574707031, 358.88531494140625, 358.9221496582031, 359.0246887207031, 358.8587646484375, 358.8844909667969, 359.1251220703125, 358.9252624511719, 358.91473388671875], [331.7243957519531, 331.76104736328125, 331.5594482421875, 331.6163635253906, 331.6883239746094, 331.7227783203125, 331.6319885253906, 331.7158508300781, 331.6869812011719, 331.6981201171875, 331.702880859375], [427.5985107421875, 427.578125, 427.5513000488281, 427.60650634765625, 427.54937744140625, 427.51812744140625, 427.69793701171875, 427.7482604980469, 427.61883544921875, 427.53173828125, 427.5723571777344], [388.0020751953125, 388.0804138183594, 388.09307861328125, 388.1541748046875, 388.1396179199219, 388.207275390625, 388.0601501464844, 388.2804870605469, 388.0313415527344, 388.1189270019531, 388.2029724121094], [404.8681640625, 405.1084899902344, 404.8268737792969, 404.9685363769531, 404.9539489746094, 404.9624938964844, 404.9993591308594, 404.7662353515625, 405.0172424316406, 405.3266906738281, 404.91656494140625], [515.3807983398438, 515.5010986328125, 515.293701171875, 515.3225708007812, 515.330810546875, 515.32177734375, 515.2882690429688, 515.3310546875, 515.2808227539062, 515.285888671875, 515.2958984375], [357.5610656738281, 357.620361328125, 357.4577941894531, 357.5079345703125, 357.50433349609375, 357.5034484863281, 357.5162353515625, 357.5028381347656, 357.4053955078125, 357.5231628417969, 357.474365234375], [350.6177978515625, 350.6220703125, 350.5828857421875, 350.5954284667969, 350.59246826171875, 350.5933532714844, 350.429931640625, 350.6947021484375, 350.7234191894531, 350.57080078125, 350.5740966796875], [396.4621276855469, 396.3009948730469, 396.46673583984375, 396.4918212890625, 396.48419189453125, 396.48516845703125, 396.4126281738281, 396.46881103515625, 396.6293640136719, 396.494140625, 396.5415954589844], [352.1738586425781, 352.0078125, 352.08673095703125, 352.109130859375, 352.1075439453125, 352.07403564453125, 351.948486328125, 352.2018127441406, 352.13812255859375, 352.14056396484375, 352.114013671875], [330.7941589355469, 330.909423828125, 330.7356262207031, 330.7885437011719, 330.79180908203125, 330.78155517578125, 330.7791748046875, 330.83380126953125, 330.87762451171875, 330.7593994140625, 330.74884033203125], [325.0086364746094, 325.0675048828125, 325.01531982421875, 325.0595397949219, 325.070556640625, 325.0775146484375, 324.9825134277344, 325.0307922363281, 325.00311279296875, 325.0857238769531, 325.1174621582031], [378.4460144042969, 378.4053955078125, 378.3683776855469, 378.3564147949219, 378.3372802734375, 378.37445068359375, 378.3562927246094, 378.5091552734375, 378.4736328125, 378.40704345703125, 378.327392578125], [319.5754699707031, 319.4754638671875, 319.5465087890625, 319.5434265136719, 319.5728759765625, 319.6044921875, 319.53521728515625, 319.58673095703125, 319.45404052734375, 319.53863525390625, 319.55633544921875], [352.8209533691406, 352.8044738769531, 352.7906188964844, 352.783935546875, 352.79779052734375, 352.7750244140625, 352.7585144042969, 352.76141357421875, 352.92181396484375, 352.8052062988281, 352.7921142578125], [386.13787841796875, 386.13665771484375, 386.10888671875, 386.0736083984375, 386.0834045410156, 386.1050720214844, 386.1931457519531, 386.053466796875, 386.0406799316406, 386.0872802734375, 386.1069641113281], [286.5316467285156, 286.5547790527344, 286.4952087402344, 286.37451171875, 286.53369140625, 286.4721984863281, 286.7107238769531, 286.38336181640625, 286.51220703125, 286.5129089355469, 286.53424072265625], [297.3746337890625, 297.4213562011719, 297.3291320800781, 297.34149169921875, 297.3426513671875, 297.318359375, 297.37725830078125, 297.40460205078125, 297.36083984375, 297.3843688964844, 297.3966064453125], [241.64910888671875, 241.60704040527344, 241.63507080078125, 241.671630859375, 241.66012573242188, 241.64096069335938, 241.74612426757812, 241.6597137451172, 241.6875762939453, 241.6381378173828, 241.67234802246094], [311.9956359863281, 312.0651550292969, 312.03228759765625, 312.03765869140625, 312.0518798828125, 312.0389709472656, 311.92730712890625, 311.96343994140625, 312.17645263671875, 312.03228759765625, 312.0270690917969]]}, 'test_loss_history': [4369.31517382752, 4373.30345152391, 4371.515633921831, 4371.913123363827, 4373.632872533341, 4375.521786809964, 4379.166110816405, 4371.799678950164, 4374.206023612307, 4376.3172963161005, 4370.64271747191, 4370.740000592773, 4372.4034719836345, 4373.254471576557, 4374.580885097093, 4375.079716866435, 4379.213593635075, 4374.7400293997025, 4368.623161195416, 4371.035032402882, 4374.903063091563, 4368.246109022965, 4372.926596161012, 4368.712787721503, 4367.901775625919, 4371.250809920834, 4368.611272476966, 4366.996247991832, 4370.049532344815, 4371.795392388196, 4372.863097520759, 4378.10985125946, 4367.952187905263, 4380.538059172006, 4366.6213163421335, 4366.620025700923, 4371.536474488401, 4367.9881696514985, 4370.203565723352, 4371.283469570204, 4374.730759314392, 4367.285233522133, 4376.794880138975, 4371.015884489199, 4368.600490065616, 4373.069225473129, 4376.2606304377805, 4370.964669774286, 4367.791698220714, 4364.932061916906], 'test_accuracy_history': [44.78166615051099, 67.57510065035615, 72.22050170331372, 71.63208423660576, 69.83586249612883, 66.30535769588107, 72.40631774543202, 73.6760606999071, 73.42830597708269, 73.39733663672963, 74.38835552802725, 74.6051409104986, 73.36636729637658, 75.68906782285538, 76.68008671415299, 74.66707959120471, 74.97677299473521, 90.64725921337876, 91.35955404149891, 72.87085785072777, 22.6385877980799, 23.010219882316505, 20.935274078662125, 36.45091359554041, 25.983276556209354, 29.947352121399813, 17.99318674512233, 24.34190151749768, 19.727469804893154, 19.139052338185195, 21.028182099721278, 19.634561783834005, 20.161040569835862, 21.771446268194488, 29.04924125116135, 29.235057293279652, 29.85444410034066, 27.469804893155775, 29.916382781046764, 31.55775781975844, 29.885413440693714, 11.427686590275627, 13.192938990399504, 10.21988231650666, 13.47166305357696, 10.250851656859709, 12.016104056983586, 11.087023846392071, 10.777330442861567, 8.609476618148033]}\n",
    "Model saved to d:\\herbario\\models\\pytorch/CNN/50_10_0.001_MELU_model_CNN_pytorch.pth.pt\n",
    "\n",
    "  0%|          | 0/50 [00:00<?, ?it/s]\n",
    "  2%|▏         | 1/50 [15:51<12:57:09, 951.62s/it]\n",
    "  4%|▍         | 2/50 [31:39<12:39:33, 949.44s/it]\n",
    "  6%|▌         | 3/50 [47:43<12:28:59, 956.15s/it]\n",
    "  8%|▊         | 4/50 [1:03:51<12:16:26, 960.58s/it]\n",
    " 10%|█         | 5/50 [1:19:43<11:58:14, 957.66s/it]\n",
    " 12%|█▏        | 6/50 [1:35:43<11:42:58, 958.59s/it]\n",
    " 14%|█▍        | 7/50 [1:52:20<11:35:49, 970.91s/it]\n",
    " 16%|█▌        | 8/50 [2:08:06<11:14:05, 962.98s/it]\n",
    " 18%|█▊        | 9/50 [2:23:49<10:53:47, 956.76s/it]\n",
    " 20%|██        | 10/50 [2:39:37<10:35:59, 954.00s/it]\n",
    " 22%|██▏       | 11/50 [2:55:22<10:18:17, 951.22s/it]\n",
    " 24%|██▍       | 12/50 [3:11:08<10:01:28, 949.69s/it]\n",
    " 26%|██▌       | 13/50 [3:26:52<9:44:40, 948.11s/it] \n",
    " 28%|██▊       | 14/50 [3:42:40<9:28:45, 947.92s/it]\n",
    " 30%|███       | 15/50 [3:58:25<9:12:28, 947.10s/it]\n",
    " 32%|███▏      | 16/50 [4:14:09<8:56:14, 946.32s/it]\n",
    " 34%|███▍      | 17/50 [4:29:55<8:40:18, 946.01s/it]\n",
    " 36%|███▌      | 18/50 [4:45:40<8:24:21, 945.68s/it]\n",
    " 38%|███▊      | 19/50 [5:01:25<8:08:33, 945.61s/it]\n",
    " 40%|████      | 20/50 [5:17:10<7:52:46, 945.55s/it]\n",
    " 42%|████▏     | 21/50 [5:32:50<7:36:12, 943.88s/it]\n",
    " 44%|████▍     | 22/50 [5:48:28<7:19:38, 942.09s/it]\n",
    " 46%|████▌     | 23/50 [6:04:11<7:04:02, 942.33s/it]\n",
    " 48%|████▊     | 24/50 [6:19:55<6:48:35, 942.89s/it]\n",
    " 50%|█████     | 25/50 [6:35:39<6:32:57, 943.09s/it]\n",
    " 52%|█████▏    | 26/50 [6:51:23<6:17:20, 943.34s/it]\n",
    " 54%|█████▍    | 27/50 [7:07:32<6:04:33, 951.03s/it]\n",
    " 56%|█████▌    | 28/50 [7:23:58<5:52:37, 961.69s/it]\n",
    " 58%|█████▊    | 29/50 [7:40:02<5:36:45, 962.18s/it]\n",
    " 60%|██████    | 30/50 [7:55:46<5:18:56, 956.82s/it]\n",
    " 62%|██████▏   | 31/50 [8:11:30<5:01:47, 953.03s/it]\n",
    " 64%|██████▍   | 32/50 [8:27:14<4:45:03, 950.20s/it]\n",
    " 66%|██████▌   | 33/50 [8:42:59<4:28:47, 948.66s/it]\n",
    " 68%|██████▊   | 34/50 [8:58:43<4:12:34, 947.18s/it]\n",
    " 70%|███████   | 35/50 [9:14:28<3:56:38, 946.60s/it]\n",
    " 72%|███████▏  | 36/50 [9:30:12<3:40:40, 945.77s/it]\n",
    " 74%|███████▍  | 37/50 [9:45:56<3:24:48, 945.24s/it]\n",
    " 76%|███████▌  | 38/50 [10:01:42<3:09:08, 945.69s/it]\n",
    " 78%|███████▊  | 39/50 [10:17:42<2:54:07, 949.80s/it]\n",
    " 80%|████████  | 40/50 [10:33:26<2:38:01, 948.19s/it]\n",
    " 82%|████████▏ | 41/50 [10:49:10<2:22:02, 946.98s/it]\n",
    " 84%|████████▍ | 42/50 [11:04:55<2:06:10, 946.37s/it]\n",
    " 86%|████████▌ | 43/50 [11:20:40<1:50:21, 945.96s/it]\n",
    " 88%|████████▊ | 44/50 [11:36:24<1:34:30, 945.15s/it]\n",
    " 90%|█████████ | 45/50 [11:52:08<1:18:44, 944.91s/it]\n",
    " 92%|█████████▏| 46/50 [12:07:53<1:02:59, 944.99s/it]\n",
    " 94%|█████████▍| 47/50 [12:23:37<47:14, 944.76s/it]  \n",
    " 96%|█████████▌| 48/50 [12:39:23<31:29, 944.94s/it]\n",
    " 98%|█████████▊| 49/50 [12:55:07<15:44, 944.84s/it]\n",
    "100%|██████████| 50/50 [13:10:54<00:00, 945.34s/it]\n",
    "100%|██████████| 50/50 [13:10:54<00:00, 949.09s/it]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pytorchLocal/naive_model_torch.py --epochs 50 --trainSet MELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading annotations into memory...\n",
    "Done (t=0.06s)\n",
    "creating index...\n",
    "index created!\n",
    "{'train_loss_history': {'average_loss': [2031.1336496420922, 2027.0142611585898, 2033.500202934149, 2027.9761507450626, 2034.4993800439206, 2031.0224553606836, 2026.5174923039935, 2025.7637697791085, 2028.2658852514276, 2033.5360931551395, 2028.1103308043505, 2030.7876506360049, 2030.1829019149548, 2026.665217830445, 2031.1357601572415, 2030.0693055748334, 2034.4407113167235, 2029.0526804657757, 2022.1595316465737, 2029.0836723831098, 2031.840198807305, 2026.7085060080901, 2032.0959119457884, 2025.4824575046598, 2030.6153657400064, 2032.100095177665, 2028.9513013810677, 2033.4077644154504, 2034.4319894858422, 2024.6482890947216, 2030.465547668147, 2027.1063663076022, 2026.380898102891, 2028.297186682067, 2028.5937152998097, 2031.6675374762056, 2028.3418387011222, 2034.1495317952888, 2020.2437285602396, 2028.7391605280377, 2029.4356215423738, 2029.2182372427228, 2024.322497682523, 2028.9761795586135, 2020.8320820609931, 2031.5906619928815, 2029.9130685874047, 2029.4087307537873, 2029.5025117361001, 2027.3264358443053], 'loss_bbox_per_class': [[tensor(0.7543, grad_fn=<RsubBackward1>), tensor(0.7427, grad_fn=<RsubBackward1>), tensor(0.7335, grad_fn=<RsubBackward1>), tensor(0.7241, grad_fn=<RsubBackward1>), tensor(0.7498, grad_fn=<RsubBackward1>), tensor(0.7577, grad_fn=<RsubBackward1>)], [tensor(0.8318, grad_fn=<RsubBackward1>), tensor(0.8168, grad_fn=<RsubBackward1>), tensor(0.8379, grad_fn=<RsubBackward1>), tensor(0.8250, grad_fn=<RsubBackward1>), tensor(0.8478, grad_fn=<RsubBackward1>), tensor(0.8407, grad_fn=<RsubBackward1>)], [tensor(0.7857, grad_fn=<RsubBackward1>), tensor(0.7888, grad_fn=<RsubBackward1>), tensor(0.7707, grad_fn=<RsubBackward1>), tensor(0.7722, grad_fn=<RsubBackward1>), tensor(0.7939, grad_fn=<RsubBackward1>), tensor(0.8042, grad_fn=<RsubBackward1>)], [tensor(0.7688, grad_fn=<RsubBackward1>), tensor(0.7539, grad_fn=<RsubBackward1>), tensor(0.7641, grad_fn=<RsubBackward1>), tensor(0.7497, grad_fn=<RsubBackward1>), tensor(0.7700, grad_fn=<RsubBackward1>), tensor(0.7722, grad_fn=<RsubBackward1>)], [tensor(0.7482, grad_fn=<RsubBackward1>), tensor(0.7284, grad_fn=<RsubBackward1>), tensor(0.7362, grad_fn=<RsubBackward1>), tensor(0.7435, grad_fn=<RsubBackward1>), tensor(0.7586, grad_fn=<RsubBackward1>), tensor(0.7521, grad_fn=<RsubBackward1>)], [tensor(0.7484, grad_fn=<RsubBackward1>), tensor(0.7267, grad_fn=<RsubBackward1>), tensor(0.7603, grad_fn=<RsubBackward1>), tensor(0.7517, grad_fn=<RsubBackward1>), tensor(0.7648, grad_fn=<RsubBackward1>), tensor(0.7556, grad_fn=<RsubBackward1>)], [tensor(0.7503, grad_fn=<RsubBackward1>), tensor(0.7267, grad_fn=<RsubBackward1>), tensor(0.7347, grad_fn=<RsubBackward1>), tensor(0.7593, grad_fn=<RsubBackward1>), tensor(0.7542, grad_fn=<RsubBackward1>), tensor(0.7629, grad_fn=<RsubBackward1>)], [tensor(0.7892, grad_fn=<RsubBackward1>), tensor(0.7807, grad_fn=<RsubBackward1>), tensor(0.7937, grad_fn=<RsubBackward1>), tensor(0.7916, grad_fn=<RsubBackward1>), tensor(0.8002, grad_fn=<RsubBackward1>), tensor(0.8000, grad_fn=<RsubBackward1>)], [tensor(0.7754, grad_fn=<RsubBackward1>), tensor(0.7357, grad_fn=<RsubBackward1>), tensor(0.7537, grad_fn=<RsubBackward1>), tensor(0.7584, grad_fn=<RsubBackward1>), tensor(0.7833, grad_fn=<RsubBackward1>), tensor(0.7686, grad_fn=<RsubBackward1>)], [tensor(0.7231, grad_fn=<RsubBackward1>), tensor(0.7285, grad_fn=<RsubBackward1>), tensor(0.7268, grad_fn=<RsubBackward1>), tensor(0.7235, grad_fn=<RsubBackward1>), tensor(0.7264, grad_fn=<RsubBackward1>), tensor(0.7623, grad_fn=<RsubBackward1>)], [tensor(0.7830, grad_fn=<RsubBackward1>), tensor(0.7465, grad_fn=<RsubBackward1>), tensor(0.7474, grad_fn=<RsubBackward1>), tensor(0.7675, grad_fn=<RsubBackward1>), tensor(0.7902, grad_fn=<RsubBackward1>), tensor(0.7759, grad_fn=<RsubBackward1>)], [tensor(0.7006, grad_fn=<RsubBackward1>), tensor(0.7072, grad_fn=<RsubBackward1>), tensor(0.7096, grad_fn=<RsubBackward1>), tensor(0.7056, grad_fn=<RsubBackward1>), tensor(0.7192, grad_fn=<RsubBackward1>), tensor(0.7447, grad_fn=<RsubBackward1>)], [tensor(0.8182, grad_fn=<RsubBackward1>), tensor(0.8136, grad_fn=<RsubBackward1>), tensor(0.8203, grad_fn=<RsubBackward1>), tensor(0.7845, grad_fn=<RsubBackward1>), tensor(0.8084, grad_fn=<RsubBackward1>), tensor(0.8207, grad_fn=<RsubBackward1>)], [tensor(0.7488, grad_fn=<RsubBackward1>), tensor(0.7383, grad_fn=<RsubBackward1>), tensor(0.7550, grad_fn=<RsubBackward1>), tensor(0.7533, grad_fn=<RsubBackward1>), tensor(0.7716, grad_fn=<RsubBackward1>), tensor(0.7682, grad_fn=<RsubBackward1>)], [tensor(0.7602, grad_fn=<RsubBackward1>), tensor(0.7640, grad_fn=<RsubBackward1>), tensor(0.7612, grad_fn=<RsubBackward1>), tensor(0.7424, grad_fn=<RsubBackward1>), tensor(0.7634, grad_fn=<RsubBackward1>), tensor(0.7827, grad_fn=<RsubBackward1>)], [tensor(0.7741, grad_fn=<RsubBackward1>), tensor(0.7570, grad_fn=<RsubBackward1>), tensor(0.7721, grad_fn=<RsubBackward1>), tensor(0.7795, grad_fn=<RsubBackward1>), tensor(0.7835, grad_fn=<RsubBackward1>), tensor(0.7706, grad_fn=<RsubBackward1>)], [tensor(0.7382, grad_fn=<RsubBackward1>), tensor(0.7126, grad_fn=<RsubBackward1>), tensor(0.7254, grad_fn=<RsubBackward1>), tensor(0.7228, grad_fn=<RsubBackward1>), tensor(0.7465, grad_fn=<RsubBackward1>), tensor(0.7387, grad_fn=<RsubBackward1>)], [tensor(0.8087, grad_fn=<RsubBackward1>), tensor(0.8040, grad_fn=<RsubBackward1>), tensor(0.8015, grad_fn=<RsubBackward1>), tensor(0.8037, grad_fn=<RsubBackward1>), tensor(0.8173, grad_fn=<RsubBackward1>), tensor(0.8182, grad_fn=<RsubBackward1>)], [tensor(0.8580, grad_fn=<RsubBackward1>), tensor(0.8488, grad_fn=<RsubBackward1>), tensor(0.8499, grad_fn=<RsubBackward1>), tensor(0.8483, grad_fn=<RsubBackward1>), tensor(0.8551, grad_fn=<RsubBackward1>), tensor(0.8590, grad_fn=<RsubBackward1>)], [tensor(0.7072, grad_fn=<RsubBackward1>), tensor(0.7058, grad_fn=<RsubBackward1>), tensor(0.6964, grad_fn=<RsubBackward1>), tensor(0.6859, grad_fn=<RsubBackward1>), tensor(0.7156, grad_fn=<RsubBackward1>), tensor(0.7326, grad_fn=<RsubBackward1>)], [tensor(0.7668, grad_fn=<RsubBackward1>), tensor(0.7627, grad_fn=<RsubBackward1>), tensor(0.7765, grad_fn=<RsubBackward1>), tensor(0.7460, grad_fn=<RsubBackward1>), tensor(0.7795, grad_fn=<RsubBackward1>), tensor(0.7747, grad_fn=<RsubBackward1>)], [tensor(0.8055, grad_fn=<RsubBackward1>), tensor(0.7948, grad_fn=<RsubBackward1>), tensor(0.7927, grad_fn=<RsubBackward1>), tensor(0.8092, grad_fn=<RsubBackward1>), tensor(0.8089, grad_fn=<RsubBackward1>), tensor(0.8325, grad_fn=<RsubBackward1>)], [tensor(0.7968, grad_fn=<RsubBackward1>), tensor(0.8111, grad_fn=<RsubBackward1>), tensor(0.8091, grad_fn=<RsubBackward1>), tensor(0.8092, grad_fn=<RsubBackward1>), tensor(0.8136, grad_fn=<RsubBackward1>), tensor(0.8144, grad_fn=<RsubBackward1>)], [tensor(0.7976, grad_fn=<RsubBackward1>), tensor(0.7587, grad_fn=<RsubBackward1>), tensor(0.7915, grad_fn=<RsubBackward1>), tensor(0.7726, grad_fn=<RsubBackward1>), tensor(0.8012, grad_fn=<RsubBackward1>), tensor(0.7975, grad_fn=<RsubBackward1>)], [tensor(0.7158, grad_fn=<RsubBackward1>), tensor(0.7408, grad_fn=<RsubBackward1>), tensor(0.7265, grad_fn=<RsubBackward1>), tensor(0.7091, grad_fn=<RsubBackward1>), tensor(0.7394, grad_fn=<RsubBackward1>), tensor(0.7538, grad_fn=<RsubBackward1>)], [tensor(0.7925, grad_fn=<RsubBackward1>), tensor(0.8033, grad_fn=<RsubBackward1>), tensor(0.7817, grad_fn=<RsubBackward1>), tensor(0.7798, grad_fn=<RsubBackward1>), tensor(0.8148, grad_fn=<RsubBackward1>), tensor(0.8103, grad_fn=<RsubBackward1>)], [tensor(0.7693, grad_fn=<RsubBackward1>), tensor(0.7287, grad_fn=<RsubBackward1>), tensor(0.7594, grad_fn=<RsubBackward1>), tensor(0.7581, grad_fn=<RsubBackward1>), tensor(0.7785, grad_fn=<RsubBackward1>), tensor(0.7791, grad_fn=<RsubBackward1>)], [tensor(0.7244, grad_fn=<RsubBackward1>), tensor(0.7190, grad_fn=<RsubBackward1>), tensor(0.7209, grad_fn=<RsubBackward1>), tensor(0.7317, grad_fn=<RsubBackward1>), tensor(0.7415, grad_fn=<RsubBackward1>), tensor(0.7408, grad_fn=<RsubBackward1>)], [tensor(0.7296, grad_fn=<RsubBackward1>), tensor(0.7171, grad_fn=<RsubBackward1>), tensor(0.7241, grad_fn=<RsubBackward1>), tensor(0.7083, grad_fn=<RsubBackward1>), tensor(0.7224, grad_fn=<RsubBackward1>), tensor(0.7329, grad_fn=<RsubBackward1>)], [tensor(0.7548, grad_fn=<RsubBackward1>), tensor(0.7361, grad_fn=<RsubBackward1>), tensor(0.7528, grad_fn=<RsubBackward1>), tensor(0.7284, grad_fn=<RsubBackward1>), tensor(0.7672, grad_fn=<RsubBackward1>), tensor(0.7689, grad_fn=<RsubBackward1>)], [tensor(0.7751, grad_fn=<RsubBackward1>), tensor(0.7919, grad_fn=<RsubBackward1>), tensor(0.8110, grad_fn=<RsubBackward1>), tensor(0.7857, grad_fn=<RsubBackward1>), tensor(0.7955, grad_fn=<RsubBackward1>), tensor(0.8026, grad_fn=<RsubBackward1>)], [tensor(0.7699, grad_fn=<RsubBackward1>), tensor(0.7603, grad_fn=<RsubBackward1>), tensor(0.7665, grad_fn=<RsubBackward1>), tensor(0.7539, grad_fn=<RsubBackward1>), tensor(0.7734, grad_fn=<RsubBackward1>), tensor(0.7674, grad_fn=<RsubBackward1>)], [tensor(0.7626, grad_fn=<RsubBackward1>), tensor(0.7430, grad_fn=<RsubBackward1>), tensor(0.7415, grad_fn=<RsubBackward1>), tensor(0.7475, grad_fn=<RsubBackward1>), tensor(0.7648, grad_fn=<RsubBackward1>), tensor(0.7648, grad_fn=<RsubBackward1>)], [tensor(0.6847, grad_fn=<RsubBackward1>), tensor(0.6764, grad_fn=<RsubBackward1>), tensor(0.7111, grad_fn=<RsubBackward1>), tensor(0.6891, grad_fn=<RsubBackward1>), tensor(0.7171, grad_fn=<RsubBackward1>), tensor(0.7154, grad_fn=<RsubBackward1>)], [tensor(0.7989, grad_fn=<RsubBackward1>), tensor(0.8056, grad_fn=<RsubBackward1>), tensor(0.8148, grad_fn=<RsubBackward1>), tensor(0.8110, grad_fn=<RsubBackward1>), tensor(0.8206, grad_fn=<RsubBackward1>), tensor(0.8129, grad_fn=<RsubBackward1>)], [tensor(0.7257, grad_fn=<RsubBackward1>), tensor(0.7342, grad_fn=<RsubBackward1>), tensor(0.7352, grad_fn=<RsubBackward1>), tensor(0.7364, grad_fn=<RsubBackward1>), tensor(0.7495, grad_fn=<RsubBackward1>), tensor(0.7566, grad_fn=<RsubBackward1>)], [tensor(0.7711, grad_fn=<RsubBackward1>), tensor(0.7818, grad_fn=<RsubBackward1>), tensor(0.7932, grad_fn=<RsubBackward1>), tensor(0.7756, grad_fn=<RsubBackward1>), tensor(0.7678, grad_fn=<RsubBackward1>), tensor(0.7930, grad_fn=<RsubBackward1>)], [tensor(0.7369, grad_fn=<RsubBackward1>), tensor(0.7312, grad_fn=<RsubBackward1>), tensor(0.7492, grad_fn=<RsubBackward1>), tensor(0.7361, grad_fn=<RsubBackward1>), tensor(0.7573, grad_fn=<RsubBackward1>), tensor(0.7539, grad_fn=<RsubBackward1>)], [tensor(0.8751, grad_fn=<RsubBackward1>), tensor(0.8791, grad_fn=<RsubBackward1>), tensor(0.8735, grad_fn=<RsubBackward1>), tensor(0.8719, grad_fn=<RsubBackward1>), tensor(0.8866, grad_fn=<RsubBackward1>), tensor(0.8769, grad_fn=<RsubBackward1>)], [tensor(0.8033, grad_fn=<RsubBackward1>), tensor(0.7888, grad_fn=<RsubBackward1>), tensor(0.8012, grad_fn=<RsubBackward1>), tensor(0.7978, grad_fn=<RsubBackward1>), tensor(0.7947, grad_fn=<RsubBackward1>), tensor(0.8010, grad_fn=<RsubBackward1>)], [tensor(0.7303, grad_fn=<RsubBackward1>), tensor(0.7375, grad_fn=<RsubBackward1>), tensor(0.7232, grad_fn=<RsubBackward1>), tensor(0.7026, grad_fn=<RsubBackward1>), tensor(0.7351, grad_fn=<RsubBackward1>), tensor(0.7474, grad_fn=<RsubBackward1>)], [tensor(0.7590, grad_fn=<RsubBackward1>), tensor(0.7758, grad_fn=<RsubBackward1>), tensor(0.7814, grad_fn=<RsubBackward1>), tensor(0.7603, grad_fn=<RsubBackward1>), tensor(0.7788, grad_fn=<RsubBackward1>), tensor(0.7739, grad_fn=<RsubBackward1>)], [tensor(0.7567, grad_fn=<RsubBackward1>), tensor(0.7458, grad_fn=<RsubBackward1>), tensor(0.7990, grad_fn=<RsubBackward1>), tensor(0.7609, grad_fn=<RsubBackward1>), tensor(0.7693, grad_fn=<RsubBackward1>), tensor(0.7707, grad_fn=<RsubBackward1>)], [tensor(0.7514, grad_fn=<RsubBackward1>), tensor(0.7467, grad_fn=<RsubBackward1>), tensor(0.7600, grad_fn=<RsubBackward1>), tensor(0.7558, grad_fn=<RsubBackward1>), tensor(0.7820, grad_fn=<RsubBackward1>), tensor(0.7822, grad_fn=<RsubBackward1>)], [tensor(0.7840, grad_fn=<RsubBackward1>), tensor(0.7586, grad_fn=<RsubBackward1>), tensor(0.7741, grad_fn=<RsubBackward1>), tensor(0.7683, grad_fn=<RsubBackward1>), tensor(0.7927, grad_fn=<RsubBackward1>), tensor(0.7795, grad_fn=<RsubBackward1>)], [tensor(0.8143, grad_fn=<RsubBackward1>), tensor(0.8158, grad_fn=<RsubBackward1>), tensor(0.8361, grad_fn=<RsubBackward1>), tensor(0.8222, grad_fn=<RsubBackward1>), tensor(0.8405, grad_fn=<RsubBackward1>), tensor(0.8294, grad_fn=<RsubBackward1>)], [tensor(0.7803, grad_fn=<RsubBackward1>), tensor(0.7400, grad_fn=<RsubBackward1>), tensor(0.7580, grad_fn=<RsubBackward1>), tensor(0.7724, grad_fn=<RsubBackward1>), tensor(0.7734, grad_fn=<RsubBackward1>), tensor(0.7694, grad_fn=<RsubBackward1>)], [tensor(0.8147, grad_fn=<RsubBackward1>), tensor(0.7959, grad_fn=<RsubBackward1>), tensor(0.8082, grad_fn=<RsubBackward1>), tensor(0.8044, grad_fn=<RsubBackward1>), tensor(0.8232, grad_fn=<RsubBackward1>), tensor(0.8129, grad_fn=<RsubBackward1>)], [tensor(0.7606, grad_fn=<RsubBackward1>), tensor(0.7515, grad_fn=<RsubBackward1>), tensor(0.7703, grad_fn=<RsubBackward1>), tensor(0.7356, grad_fn=<RsubBackward1>), tensor(0.7983, grad_fn=<RsubBackward1>), tensor(0.7807, grad_fn=<RsubBackward1>)], [tensor(0.7491, grad_fn=<RsubBackward1>), tensor(0.7482, grad_fn=<RsubBackward1>), tensor(0.7581, grad_fn=<RsubBackward1>), tensor(0.7585, grad_fn=<RsubBackward1>), tensor(0.7668, grad_fn=<RsubBackward1>), tensor(0.7788, grad_fn=<RsubBackward1>)]], 'loss_class_per_class': [[tensor(145.0170, grad_fn=<DivBackward1>), tensor(144.1494, grad_fn=<DivBackward1>), tensor(144.1585, grad_fn=<DivBackward1>), tensor(143.3072, grad_fn=<DivBackward1>), tensor(144.4230, grad_fn=<DivBackward1>), tensor(144.1800, grad_fn=<DivBackward1>)], [tensor(199.3644, grad_fn=<DivBackward1>), tensor(199.4769, grad_fn=<DivBackward1>), tensor(199.5483, grad_fn=<DivBackward1>), tensor(199.6184, grad_fn=<DivBackward1>), tensor(199.7465, grad_fn=<DivBackward1>), tensor(199.5622, grad_fn=<DivBackward1>)], [tensor(153.0957, grad_fn=<DivBackward1>), tensor(153.1180, grad_fn=<DivBackward1>), tensor(153.0996, grad_fn=<DivBackward1>), tensor(152.9282, grad_fn=<DivBackward1>), tensor(152.6362, grad_fn=<DivBackward1>), tensor(153.1176, grad_fn=<DivBackward1>)], [tensor(155.0909, grad_fn=<DivBackward1>), tensor(154.8162, grad_fn=<DivBackward1>), tensor(154.7857, grad_fn=<DivBackward1>), tensor(155.2670, grad_fn=<DivBackward1>), tensor(154.0421, grad_fn=<DivBackward1>), tensor(154.8020, grad_fn=<DivBackward1>)], [tensor(109.3418, grad_fn=<DivBackward1>), tensor(110.0010, grad_fn=<DivBackward1>), tensor(109.9596, grad_fn=<DivBackward1>), tensor(109.7715, grad_fn=<DivBackward1>), tensor(108.8909, grad_fn=<DivBackward1>), tensor(109.9748, grad_fn=<DivBackward1>)], [tensor(105.1034, grad_fn=<DivBackward1>), tensor(105.1270, grad_fn=<DivBackward1>), tensor(105.1722, grad_fn=<DivBackward1>), tensor(105.2574, grad_fn=<DivBackward1>), tensor(105.1837, grad_fn=<DivBackward1>), tensor(105.1805, grad_fn=<DivBackward1>)], [tensor(110.2773, grad_fn=<DivBackward1>), tensor(109.9526, grad_fn=<DivBackward1>), tensor(109.8950, grad_fn=<DivBackward1>), tensor(109.5862, grad_fn=<DivBackward1>), tensor(109.7486, grad_fn=<DivBackward1>), tensor(109.9084, grad_fn=<DivBackward1>)], [tensor(135.4998, grad_fn=<DivBackward1>), tensor(136.4875, grad_fn=<DivBackward1>), tensor(136.4244, grad_fn=<DivBackward1>), tensor(135.8664, grad_fn=<DivBackward1>), tensor(135.9122, grad_fn=<DivBackward1>), tensor(136.4376, grad_fn=<DivBackward1>)], [tensor(174.8681, grad_fn=<DivBackward1>), tensor(174.9901, grad_fn=<DivBackward1>), tensor(174.9235, grad_fn=<DivBackward1>), tensor(174.8736, grad_fn=<DivBackward1>), tensor(174.8101, grad_fn=<DivBackward1>), tensor(174.9355, grad_fn=<DivBackward1>)], [tensor(118.6460, grad_fn=<DivBackward1>), tensor(118.9951, grad_fn=<DivBackward1>), tensor(118.9254, grad_fn=<DivBackward1>), tensor(118.5192, grad_fn=<DivBackward1>), tensor(118.3342, grad_fn=<DivBackward1>), tensor(118.9370, grad_fn=<DivBackward1>)], [tensor(147.5615, grad_fn=<DivBackward1>), tensor(147.6958, grad_fn=<DivBackward1>), tensor(147.7276, grad_fn=<DivBackward1>), tensor(147.5039, grad_fn=<DivBackward1>), tensor(147.7650, grad_fn=<DivBackward1>), tensor(147.7322, grad_fn=<DivBackward1>)], [tensor(129.4853, grad_fn=<DivBackward1>), tensor(129.6808, grad_fn=<DivBackward1>), tensor(129.7129, grad_fn=<DivBackward1>), tensor(129.7976, grad_fn=<DivBackward1>), tensor(129.2943, grad_fn=<DivBackward1>), tensor(129.7165, grad_fn=<DivBackward1>)], [tensor(166.7900, grad_fn=<DivBackward1>), tensor(167.3621, grad_fn=<DivBackward1>), tensor(167.2895, grad_fn=<DivBackward1>), tensor(167.4394, grad_fn=<DivBackward1>), tensor(166.8534, grad_fn=<DivBackward1>), tensor(167.2995, grad_fn=<DivBackward1>)], [tensor(132.9999, grad_fn=<DivBackward1>), tensor(133.1089, grad_fn=<DivBackward1>), tensor(133.0383, grad_fn=<DivBackward1>), tensor(133.2164, grad_fn=<DivBackward1>), tensor(133.3175, grad_fn=<DivBackward1>), tensor(133.0476, grad_fn=<DivBackward1>)], [tensor(143.2857, grad_fn=<DivBackward1>), tensor(143.8327, grad_fn=<DivBackward1>), tensor(143.7652, grad_fn=<DivBackward1>), tensor(143.3899, grad_fn=<DivBackward1>), tensor(143.3848, grad_fn=<DivBackward1>), tensor(143.7738, grad_fn=<DivBackward1>)], [tensor(166.9850, grad_fn=<DivBackward1>), tensor(167.2994, grad_fn=<DivBackward1>), tensor(167.2329, grad_fn=<DivBackward1>), tensor(167.1181, grad_fn=<DivBackward1>), tensor(167.2784, grad_fn=<DivBackward1>), tensor(167.2413, grad_fn=<DivBackward1>)], [tensor(118.6979, grad_fn=<DivBackward1>), tensor(118.8559, grad_fn=<DivBackward1>), tensor(118.7945, grad_fn=<DivBackward1>), tensor(118.6636, grad_fn=<DivBackward1>), tensor(118.4148, grad_fn=<DivBackward1>), tensor(118.8021, grad_fn=<DivBackward1>)], [tensor(141.5986, grad_fn=<DivBackward1>), tensor(141.7918, grad_fn=<DivBackward1>), tensor(141.7318, grad_fn=<DivBackward1>), tensor(142.0124, grad_fn=<DivBackward1>), tensor(142.1332, grad_fn=<DivBackward1>), tensor(141.7391, grad_fn=<DivBackward1>)], [tensor(187.6289, grad_fn=<DivBackward1>), tensor(187.5119, grad_fn=<DivBackward1>), tensor(187.4562, grad_fn=<DivBackward1>), tensor(187.6629, grad_fn=<DivBackward1>), tensor(186.9896, grad_fn=<DivBackward1>), tensor(187.4630, grad_fn=<DivBackward1>)], [tensor(129.2816, grad_fn=<DivBackward1>), tensor(129.2605, grad_fn=<DivBackward1>), tensor(129.2116, grad_fn=<DivBackward1>), tensor(129.1665, grad_fn=<DivBackward1>), tensor(129.3273, grad_fn=<DivBackward1>), tensor(129.2175, grad_fn=<DivBackward1>)], [tensor(178.2258, grad_fn=<DivBackward1>), tensor(178.4627, grad_fn=<DivBackward1>), tensor(178.4150, grad_fn=<DivBackward1>), tensor(178.3535, grad_fn=<DivBackward1>), tensor(178.4822, grad_fn=<DivBackward1>), tensor(178.4209, grad_fn=<DivBackward1>)], [tensor(147.7547, grad_fn=<DivBackward1>), tensor(147.6730, grad_fn=<DivBackward1>), tensor(147.6975, grad_fn=<DivBackward1>), tensor(147.7668, grad_fn=<DivBackward1>), tensor(147.5568, grad_fn=<DivBackward1>), tensor(147.6986, grad_fn=<DivBackward1>)], [tensor(162.2009, grad_fn=<DivBackward1>), tensor(162.0209, grad_fn=<DivBackward1>), tensor(162.0438, grad_fn=<DivBackward1>), tensor(162.0088, grad_fn=<DivBackward1>), tensor(161.8685, grad_fn=<DivBackward1>), tensor(162.0449, grad_fn=<DivBackward1>)], [tensor(128.2276, grad_fn=<DivBackward1>), tensor(128.0363, grad_fn=<DivBackward1>), tensor(128.0594, grad_fn=<DivBackward1>), tensor(127.9518, grad_fn=<DivBackward1>), tensor(127.9232, grad_fn=<DivBackward1>), tensor(128.0603, grad_fn=<DivBackward1>)], [tensor(119.1269, grad_fn=<DivBackward1>), tensor(119.2215, grad_fn=<DivBackward1>), tensor(119.2453, grad_fn=<DivBackward1>), tensor(119.1926, grad_fn=<DivBackward1>), tensor(119.1414, grad_fn=<DivBackward1>), tensor(119.2458, grad_fn=<DivBackward1>)], [tensor(190.7142, grad_fn=<DivBackward1>), tensor(190.5149, grad_fn=<DivBackward1>), tensor(190.4823, grad_fn=<DivBackward1>), tensor(190.4530, grad_fn=<DivBackward1>), tensor(190.6974, grad_fn=<DivBackward1>), tensor(190.4866, grad_fn=<DivBackward1>)], [tensor(158.0862, grad_fn=<DivBackward1>), tensor(158.1874, grad_fn=<DivBackward1>), tensor(158.1604, grad_fn=<DivBackward1>), tensor(158.0925, grad_fn=<DivBackward1>), tensor(158.2047, grad_fn=<DivBackward1>), tensor(158.1641, grad_fn=<DivBackward1>)], [tensor(119.4548, grad_fn=<DivBackward1>), tensor(119.7241, grad_fn=<DivBackward1>), tensor(119.7030, grad_fn=<DivBackward1>), tensor(119.5954, grad_fn=<DivBackward1>), tensor(119.8072, grad_fn=<DivBackward1>), tensor(119.7060, grad_fn=<DivBackward1>)], [tensor(118.6376, grad_fn=<DivBackward1>), tensor(118.7133, grad_fn=<DivBackward1>), tensor(118.6912, grad_fn=<DivBackward1>), tensor(118.7860, grad_fn=<DivBackward1>), tensor(118.9701, grad_fn=<DivBackward1>), tensor(118.6943, grad_fn=<DivBackward1>)], [tensor(132.7583, grad_fn=<DivBackward1>), tensor(132.8978, grad_fn=<DivBackward1>), tensor(132.8768, grad_fn=<DivBackward1>), tensor(132.6746, grad_fn=<DivBackward1>), tensor(132.8083, grad_fn=<DivBackward1>), tensor(132.8798, grad_fn=<DivBackward1>)], [tensor(136.3895, grad_fn=<DivBackward1>), tensor(136.1417, grad_fn=<DivBackward1>), tensor(136.1214, grad_fn=<DivBackward1>), tensor(136.0583, grad_fn=<DivBackward1>), tensor(136.2776, grad_fn=<DivBackward1>), tensor(136.1244, grad_fn=<DivBackward1>)], [tensor(144.3184, grad_fn=<DivBackward1>), tensor(144.1697, grad_fn=<DivBackward1>), tensor(144.1871, grad_fn=<DivBackward1>), tensor(144.4727, grad_fn=<DivBackward1>), tensor(144.2751, grad_fn=<DivBackward1>), tensor(144.1874, grad_fn=<DivBackward1>)], [tensor(104.9934, grad_fn=<DivBackward1>), tensor(105.0867, grad_fn=<DivBackward1>), tensor(105.1040, grad_fn=<DivBackward1>), tensor(105.0162, grad_fn=<DivBackward1>), tensor(105.0213, grad_fn=<DivBackward1>), tensor(105.1041, grad_fn=<DivBackward1>)], [tensor(134.4693, grad_fn=<DivBackward1>), tensor(134.3593, grad_fn=<DivBackward1>), tensor(134.3473, grad_fn=<DivBackward1>), tensor(134.4242, grad_fn=<DivBackward1>), tensor(134.4250, grad_fn=<DivBackward1>), tensor(134.3494, grad_fn=<DivBackward1>)], [tensor(166.9700, grad_fn=<DivBackward1>), tensor(167.0942, grad_fn=<DivBackward1>), tensor(167.0811, grad_fn=<DivBackward1>), tensor(167.1022, grad_fn=<DivBackward1>), tensor(167.1891, grad_fn=<DivBackward1>), tensor(167.0832, grad_fn=<DivBackward1>)], [tensor(110.0418, grad_fn=<DivBackward1>), tensor(110.1394, grad_fn=<DivBackward1>), tensor(110.1544, grad_fn=<DivBackward1>), tensor(110.1436, grad_fn=<DivBackward1>), tensor(110.2111, grad_fn=<DivBackward1>), tensor(110.1545, grad_fn=<DivBackward1>)], [tensor(138.3031, grad_fn=<DivBackward1>), tensor(138.2415, grad_fn=<DivBackward1>), tensor(138.2309, grad_fn=<DivBackward1>), tensor(138.1921, grad_fn=<DivBackward1>), tensor(138.3217, grad_fn=<DivBackward1>), tensor(138.2327, grad_fn=<DivBackward1>)], [tensor(110.3412, grad_fn=<DivBackward1>), tensor(110.1392, grad_fn=<DivBackward1>), tensor(110.1525, grad_fn=<DivBackward1>), tensor(110.0486, grad_fn=<DivBackward1>), tensor(110.0358, grad_fn=<DivBackward1>), tensor(110.1526, grad_fn=<DivBackward1>)], [tensor(205.4131, grad_fn=<DivBackward1>), tensor(205.6271, grad_fn=<DivBackward1>), tensor(205.6372, grad_fn=<DivBackward1>), tensor(205.7911, grad_fn=<DivBackward1>), tensor(205.7697, grad_fn=<DivBackward1>), tensor(205.6376, grad_fn=<DivBackward1>)], [tensor(122.7172, grad_fn=<DivBackward1>), tensor(122.6697, grad_fn=<DivBackward1>), tensor(122.6798, grad_fn=<DivBackward1>), tensor(122.6181, grad_fn=<DivBackward1>), tensor(122.6259, grad_fn=<DivBackward1>), tensor(122.6801, grad_fn=<DivBackward1>)], [tensor(115.2494, grad_fn=<DivBackward1>), tensor(115.1941, grad_fn=<DivBackward1>), tensor(115.2056, grad_fn=<DivBackward1>), tensor(115.4201, grad_fn=<DivBackward1>), tensor(115.1684, grad_fn=<DivBackward1>), tensor(115.2057, grad_fn=<DivBackward1>)], [tensor(127.9799, grad_fn=<DivBackward1>), tensor(128.0472, grad_fn=<DivBackward1>), tensor(128.0564, grad_fn=<DivBackward1>), tensor(127.8641, grad_fn=<DivBackward1>), tensor(128.0721, grad_fn=<DivBackward1>), tensor(128.0567, grad_fn=<DivBackward1>)], [tensor(143.4476, grad_fn=<DivBackward1>), tensor(143.5973, grad_fn=<DivBackward1>), tensor(143.5918, grad_fn=<DivBackward1>), tensor(143.5258, grad_fn=<DivBackward1>), tensor(143.6281, grad_fn=<DivBackward1>), tensor(143.5930, grad_fn=<DivBackward1>)], [tensor(142.3213, grad_fn=<DivBackward1>), tensor(142.1783, grad_fn=<DivBackward1>), tensor(142.1864, grad_fn=<DivBackward1>), tensor(142.0616, grad_fn=<DivBackward1>), tensor(142.1504, grad_fn=<DivBackward1>), tensor(142.1867, grad_fn=<DivBackward1>)], [tensor(187.6230, grad_fn=<DivBackward1>), tensor(187.3363, grad_fn=<DivBackward1>), tensor(187.3312, grad_fn=<DivBackward1>), tensor(187.3861, grad_fn=<DivBackward1>), tensor(187.3691, grad_fn=<DivBackward1>), tensor(187.3323, grad_fn=<DivBackward1>)], [tensor(156.4171, grad_fn=<DivBackward1>), tensor(156.4090, grad_fn=<DivBackward1>), tensor(156.4160, grad_fn=<DivBackward1>), tensor(156.4425, grad_fn=<DivBackward1>), tensor(156.4262, grad_fn=<DivBackward1>), tensor(156.4163, grad_fn=<DivBackward1>)], [tensor(149.4487, grad_fn=<DivBackward1>), tensor(149.5525, grad_fn=<DivBackward1>), tensor(149.5601, grad_fn=<DivBackward1>), tensor(149.5957, grad_fn=<DivBackward1>), tensor(149.5445, grad_fn=<DivBackward1>), tensor(149.5602, grad_fn=<DivBackward1>)], [tensor(142.1132, grad_fn=<DivBackward1>), tensor(142.1797, grad_fn=<DivBackward1>), tensor(142.1862, grad_fn=<DivBackward1>), tensor(142.2152, grad_fn=<DivBackward1>), tensor(142.1144, grad_fn=<DivBackward1>), tensor(142.1864, grad_fn=<DivBackward1>)], [tensor(127.9743, grad_fn=<DivBackward1>), tensor(128.0499, grad_fn=<DivBackward1>), tensor(128.0562, grad_fn=<DivBackward1>), tensor(128.1666, grad_fn=<DivBackward1>), tensor(128.1543, grad_fn=<DivBackward1>), tensor(128.0564, grad_fn=<DivBackward1>)], [tensor(152.7414, grad_fn=<DivBackward1>), tensor(152.5942, grad_fn=<DivBackward1>), tensor(152.5915, grad_fn=<DivBackward1>), tensor(152.2777, grad_fn=<DivBackward1>), tensor(152.7674, grad_fn=<DivBackward1>), tensor(152.5922, grad_fn=<DivBackward1>)]]}, 'test_loss_history': [2033.5501811226006, 2033.6790316044376, 2029.6490134611952, 2031.5362530238738, 2026.2123211700905, 2027.3617940989848, 2026.696729321165, 2032.0951891284303, 2028.6153496292036, 2028.4551131350136, 2029.1086224396217, 2028.2350485554807, 2027.5048059763642, 2027.7051010519115, 2034.1902729169972, 2028.345224448267, 2030.530230062262, 2024.3568053269748, 2032.6255802987191, 2030.2051633263602, 2030.2975944093037, 2025.759826350333, 2023.5423147133765, 2025.6801692749643, 2029.0326507103625, 2032.5287063521178, 2031.864712013206, 2025.9162727781963, 2030.9823936810953, 2031.3318125148714, 2028.7050065558574, 2028.6531393757932, 2026.747592674294, 2026.6763637174809, 2025.8939066465737, 2030.5595067987588, 2027.1698323361159, 2033.8453672767291, 2026.0491429052981, 2031.1748015892688, 2026.5953641784977, 2028.690780097458, 2025.285470410652, 2024.9501042245004, 2025.7746563441863, 2027.5131510623216, 2027.2948214923065, 2025.177263134022, 2031.8031098806314, 2024.6173805198089], 'test_accuracy_history': [0.0, 0.0, 10.050761421319796, 10.050761421319796, 10.050761421319796, 10.050761421319796, 10.050761421319796, 10.050761421319796, 10.050761421319796, 10.050761421319796, 10.050761421319796, 10.050761421319796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
    "Model saved to d:\\herbario\\models\\pytorch/CNN/50_10_0.001_UN_model_CNN_pytorch.pth.pt\n",
    "\n",
    "  0%|          | 0/50 [00:00<?, ?it/s]\n",
    "  2%|▏         | 1/50 [01:44<1:24:57, 104.02s/it]\n",
    "  4%|▍         | 2/50 [03:27<1:22:52, 103.60s/it]\n",
    "  6%|▌         | 3/50 [05:10<1:21:05, 103.52s/it]\n",
    "  8%|▊         | 4/50 [06:53<1:19:15, 103.39s/it]\n",
    " 10%|█         | 5/50 [08:37<1:17:27, 103.28s/it]\n",
    " 12%|█▏        | 6/50 [10:20<1:15:44, 103.28s/it]\n",
    " 14%|█▍        | 7/50 [12:04<1:14:15, 103.62s/it]\n",
    " 16%|█▌        | 8/50 [13:48<1:12:37, 103.75s/it]\n",
    " 18%|█▊        | 9/50 [15:33<1:11:10, 104.17s/it]\n",
    " 20%|██        | 10/50 [17:18<1:09:30, 104.26s/it]\n",
    " 22%|██▏       | 11/50 [19:02<1:07:47, 104.29s/it]\n",
    " 24%|██▍       | 12/50 [20:48<1:06:23, 104.82s/it]\n",
    " 26%|██▌       | 13/50 [22:33<1:04:39, 104.84s/it]\n",
    " 28%|██▊       | 14/50 [24:14<1:02:08, 103.58s/it]\n",
    " 30%|███       | 15/50 [25:53<59:44, 102.41s/it]  \n",
    " 32%|███▏      | 16/50 [27:33<57:36, 101.65s/it]\n",
    " 34%|███▍      | 17/50 [29:13<55:37, 101.13s/it]\n",
    " 36%|███▌      | 18/50 [30:53<53:41, 100.66s/it]\n",
    " 38%|███▊      | 19/50 [32:32<51:52, 100.39s/it]\n",
    " 40%|████      | 20/50 [34:12<50:03, 100.13s/it]\n",
    " 42%|████▏     | 21/50 [35:52<48:22, 100.09s/it]\n",
    " 44%|████▍     | 22/50 [37:31<46:35, 99.85s/it] \n",
    " 46%|████▌     | 23/50 [39:10<44:50, 99.63s/it]\n",
    " 48%|████▊     | 24/50 [40:50<43:07, 99.51s/it]\n",
    " 50%|█████     | 25/50 [42:29<41:30, 99.60s/it]\n",
    " 52%|█████▏    | 26/50 [44:09<39:51, 99.66s/it]\n",
    " 54%|█████▍    | 27/50 [45:49<38:09, 99.55s/it]\n",
    " 56%|█████▌    | 28/50 [47:28<36:30, 99.55s/it]\n",
    " 58%|█████▊    | 29/50 [49:08<34:50, 99.55s/it]\n",
    " 60%|██████    | 30/50 [50:48<33:17, 99.88s/it]\n",
    " 62%|██████▏   | 31/50 [52:28<31:39, 99.95s/it]\n",
    " 64%|██████▍   | 32/50 [54:09<30:02, 100.14s/it]\n",
    " 66%|██████▌   | 33/50 [55:49<28:20, 100.05s/it]\n",
    " 68%|██████▊   | 34/50 [57:29<26:41, 100.11s/it]\n",
    " 70%|███████   | 35/50 [59:09<24:59, 99.95s/it] \n",
    " 72%|███████▏  | 36/50 [1:00:49<23:19, 99.95s/it]\n",
    " 74%|███████▍  | 37/50 [1:02:28<21:36, 99.69s/it]\n",
    " 76%|███████▌  | 38/50 [1:04:07<19:55, 99.62s/it]\n",
    " 78%|███████▊  | 39/50 [1:05:47<18:15, 99.60s/it]\n",
    " 80%|████████  | 40/50 [1:07:26<16:35, 99.57s/it]\n",
    " 82%|████████▏ | 41/50 [1:09:07<14:59, 99.91s/it]\n",
    " 84%|████████▍ | 42/50 [1:10:47<13:18, 99.83s/it]\n",
    " 86%|████████▌ | 43/50 [1:12:25<11:36, 99.46s/it]\n",
    " 88%|████████▊ | 44/50 [1:14:04<09:56, 99.34s/it]\n",
    " 90%|█████████ | 45/50 [1:15:43<08:16, 99.21s/it]\n",
    " 92%|█████████▏| 46/50 [1:17:23<06:37, 99.39s/it]\n",
    " 94%|█████████▍| 47/50 [1:19:04<04:59, 99.83s/it]\n",
    " 96%|█████████▌| 48/50 [1:20:43<03:19, 99.66s/it]\n",
    " 98%|█████████▊| 49/50 [1:22:22<01:39, 99.46s/it]\n",
    "100%|██████████| 50/50 [1:24:01<00:00, 99.43s/it]\n",
    "100%|██████████| 50/50 [1:24:01<00:00, 100.84s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# config_object = open(,'rb')\n",
    "# network_structure = json.load(config_object)\n",
    "\n",
    "# max_n_boxes = get_maximum_number_of_annotation_in_set(annotations,images) \n",
    "# coco_dataset = CustomCocoDetection(IMAGES_FOLDER,COCO_ANNOTATION_FILE,transform=TRANSFORMS)\n",
    "# data_loader = DataLoader(coco_dataset, batch_size = args.batch_size, shuffle=True,collate_fn = custom_collate_fn )\n",
    "\n",
    "\n",
    "# num_epochs = args.epochs\n",
    "# learning_rate = args.learning_rate\n",
    "\n",
    "  \n",
    "# BasicModel = ModelFromScratch(network_structure,num_classes,max_n_boxes,IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meluModelPytorchWeigths.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstate = meluModelPytorch.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstate.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchLocal.models_torch import ModelFromScratch\n",
    "from pytorchLocal.utils_torch import get_maximum_number_of_annotation_in_set\n",
    "\n",
    "config_object = open(get_project_configs('json/torch_simple.json'),'rb')\n",
    "network_structure = json.load(config_object)\n",
    "\n",
    "annotationsMelu = cnf_melu['annotations']\n",
    "imagesMelu = cnf_melu['images']\n",
    "\n",
    "annotationsUN = cngf_un['annotations']\n",
    "imagesUN = cngf_un['images']\n",
    "\n",
    "max_n_boxesUN = get_maximum_number_of_annotation_in_set(annotationsUN,imagesUN) \n",
    "UNModelPytorch = ModelFromScratch(network_structure,num_clases_un,max_n_boxesUN,IMG_SHAPE)\n",
    "UNModelPytorch.load_state_dict(UnModelPytorchWeigths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluateModelMeluPytorch = PytorchEvaluator(meluModelPytorch,[path_single_image_to_predict_unal],'mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModelMeluTensorflow.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = scanned_model.predict(image_array_predict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision modelos cruzados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unal_image_name = [x for x in os.listdir(image_folder_unal) if x.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melu_image_name = [x for x in os.listdir(image_folder_melu) if x.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "# from torch import model\n",
    "import os\n",
    "# Model\n",
    "model_yolov8_state_dict_melu = torch.load(os.path.join(os.getcwd(),'yolo_trained_melu','last.pt'))\n",
    "# model.load_state_dict(model_yolov8_state_dict)\n",
    "model_melu = YOLO(os.path.join(os.getcwd(),'yolo_trained_melu','last.pt'))\n",
    "model_unal = YOLO(os.path.join(os.getcwd(),'yolo_trained_unal','last_unal.pt'))\n",
    "\n",
    "# Images\n",
    "# imgs=['E:\\Study\\currency.jpg']\n",
    "\n",
    "# # Inference\n",
    "# results = model(imgs)\n",
    "\n",
    "# # Results\n",
    "# results.print()\n",
    "# results.save()  # or .show()\n",
    "# results.show()\n",
    "# results.xyxy[0]  # img1 predictions (tensor)\n",
    "# results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for image_unal in tqdm(unal_image_name[:20],total = len(unal_image_name[:20])):\n",
    "    predictions_yolo_melu_on_unal = model_melu.predict(source = os.path.join(image_folder_unal,image_unal), save = True ,save_txt = True)\n",
    "    predictions_yolo_unal_on_unal = model_unal.predict(source = os.path.join(image_folder_unal,image_unal), save = True ,save_txt = True)\n",
    "# predictions_yolo_unal_on_melu = model_unal.predict(source = path_image_to_predict_melu, save = True ,save_txt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for image_melu in tqdm(melu_image_name[:20],total = len(melu_image_name[:20])):\n",
    "    # predictions_yolo_melu_on_melu = model_melu.predict(source = os.path.join(image_folder_melu,image_melu), save = True ,save_txt = True)\n",
    "    predictions_yolo_unal_on_unal = model_unal.predict(source = os.path.join(image_folder_melu,image_melu), save = True ,save_txt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_true_labels_path = os.path.join(os.getcwd(),'data','labels')\n",
    "ground_true_labels_files = os.listdir(ground_true_labels_path)\n",
    "\n",
    "prediction_labels_path = os.path.join(os.getcwd(),'runs','detect','predict','labels')\n",
    "prediction_labels_files = os.listdir(prediction_labels_path)\n",
    "\n",
    "# def txt_file_information_colector(filename:str)->dict:\n",
    "#     '''\n",
    "#     function dedicated to read the txt files that contains the annotations per images \n",
    "#     (ground/prediction) an return it as a dict\n",
    "#     '''\n",
    "#     image_annotations = {\n",
    "#         'categories' : [],\n",
    "#         'v1' : [],\n",
    "#         'v2' : [],\n",
    "#         'v3' : [],\n",
    "#         'v4' : [],\n",
    "#     }\n",
    "\n",
    "#     with open(filename) as file:\n",
    "#         lines = file.readlines()\n",
    "        \n",
    "#         for line in lines:\n",
    "#             value_list = line.split(' ')\n",
    "        \n",
    "#             for key,value in zip(image_annotations.keys(),value_list):\n",
    "#                 image_annotations[key].append(value.replace('\\n',''))\n",
    "\n",
    "#     return image_annotations\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import txt_file_information_colector,diff_annotations\n",
    "from tqdm import tqdm \n",
    "\n",
    "ground_true_labels_path = os.path.join(os.getcwd(),'runs','detect','predict2','labels')\n",
    "ground_true_labels_files = os.listdir(ground_true_labels_path)\n",
    "\n",
    "prediction_labels_path = os.path.join(os.getcwd(),'runs','detect','predict3','labels')\n",
    "prediction_labels_files = os.listdir(prediction_labels_path)\n",
    "\n",
    "similarity_scores = []\n",
    "\n",
    "for prediction in tqdm(prediction_labels_files,total = len(prediction_labels_files)):\n",
    "    ann_p = txt_file_information_colector(os.path.join(prediction_labels_path,prediction))\n",
    "    ann_g = txt_file_information_colector(os.path.join(ground_true_labels_path,prediction))\n",
    "    score = diff_annotations(ann_p,ann_g)\n",
    "    similarity_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(similarity_scores)/len(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_annotations(ann_p,ann_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_yolo_melu_on_unal = model_melu.predict(source = path_image_to_predict_unal, save = True ,save_txt = True)\n",
    "predictions_yolo_unal_on_melu = model_unal.predict(source = path_image_to_predict_melu, save = True ,save_txt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_yolo_melu_on_melu = model_melu.predict(source = path_image_to_predict_melu, save = True ,save_txt = True)\n",
    "predictions_yolo_unal_on_unal = model_unal.predict(source = path_image_to_predict_unal, save = True ,save_txt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arquitectura de consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, add, concatenate\n",
    "# from tensorflow.keras.layers.merge import add, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import struct\n",
    "import cv2\n",
    "\n",
    "np.set_printoptions(threshold= 1000 )#np.nan)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "argparser = argparse.ArgumentParser(\n",
    "    description='test yolov3 network with coco weights')\n",
    "\n",
    "argparser.add_argument(\n",
    "    '-w',\n",
    "    '--weights',\n",
    "    help='path to weights file')\n",
    "\n",
    "argparser.add_argument(\n",
    "    '-i',\n",
    "    '--image',\n",
    "    help='path to image file')\n",
    "\n",
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major,    = struct.unpack('i', w_f.read(4))\n",
    "            minor,    = struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "\n",
    "            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                w_f.read(4)\n",
    "\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            \n",
    "            binary = w_f.read()\n",
    "\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "\n",
    "    def load_weights(self, model):\n",
    "        for i in range(106):\n",
    "            try:\n",
    "                conv_layer = model.get_layer('conv_' + str(i))\n",
    "                print(\"loading weights of convolution #\" + str(i))\n",
    "\n",
    "                if i not in [81, 93, 105]:\n",
    "                    norm_layer = model.get_layer('bnorm_' + str(i))\n",
    "\n",
    "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "\n",
    "                    beta  = self.read_bytes(size) # bias\n",
    "                    gamma = self.read_bytes(size) # scale\n",
    "                    mean  = self.read_bytes(size) # mean\n",
    "                    var   = self.read_bytes(size) # variance            \n",
    "\n",
    "                    weights = norm_layer.set_weights([gamma, beta, mean, var])  \n",
    "\n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    \n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel, bias])\n",
    "                else:\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel])\n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i))     \n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 0\n",
    "\n",
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        \n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "            \n",
    "        return self.score\n",
    "\n",
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    \n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        \n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'], \n",
    "                   conv['kernel'], \n",
    "                   strides=conv['stride'], \n",
    "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                   name='conv_' + str(conv['layer_idx']), \n",
    "                   use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "\n",
    "    return add([skip_connection, x]) if skip else x\n",
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3          \n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union\n",
    "\n",
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(None, None, 3))\n",
    "\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "        \n",
    "    skip_36 = x\n",
    "        \n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "        \n",
    "    skip_61 = x\n",
    "        \n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "\n",
    "    # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "        \n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "\n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
    "                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "\n",
    "    # Layer 83 => 86\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_61])\n",
    "\n",
    "    # Layer 87 => 91\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "\n",
    "    # Layer 92 => 94\n",
    "    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
    "                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "\n",
    "    # Layer 95 => 98\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_36])\n",
    "\n",
    "    # Layer 99 => 106\n",
    "    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
    "                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "\n",
    "    model = Model(input_image, [yolo_82, yolo_94, yolo_106])    \n",
    "    return model\n",
    "\n",
    "def preprocess_input(image, net_h, net_w):\n",
    "    new_h, new_w, _ = image.shape\n",
    "\n",
    "\n",
    "    # determine the new size of the image\n",
    "    if (float(net_w)/new_w) < (float(net_h)/new_h):\n",
    "        new_h = (new_h * net_w)/new_w\n",
    "        new_w = net_w\n",
    "    else:\n",
    "        new_w = (new_w * net_h)/new_h\n",
    "        new_h = net_h\n",
    "\n",
    "    # resize the image to the new size\n",
    "    resized = cv2.resize(image[:,:,::-1]/255., (int(new_w), int(new_h)))\n",
    "\n",
    "    # embed the image into the standard letter box\n",
    "    new_image = np.ones((net_h, net_w, 3)) * 0.5\n",
    "    new_image[int((net_h-new_h)//2):int((net_h+new_h)//2), int((net_w-new_w)//2):int((net_w+new_w)//2), :] = resized\n",
    "    new_image = np.expand_dims(new_image, 0)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "def decode_netout(netout, anchors, obj_thresh, nms_thresh, net_h, net_w):\n",
    "    grid_h, grid_w = netout.shape[:2]\n",
    "    nb_box = 3\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "\n",
    "    boxes = []\n",
    "\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    "\n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i / grid_w\n",
    "        col = i % grid_w\n",
    "        \n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[int(row)][int(col)][b][4]\n",
    "            #objectness = netout[..., :4]\n",
    "            \n",
    "            if(objectness.all() <= obj_thresh): continue\n",
    "            \n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height  \n",
    "            \n",
    "            # last elements are class probabilities\n",
    "            classes = netout[int(row)][col][b][5:]\n",
    "            \n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "            #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)\n",
    "\n",
    "            boxes.append(box)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    if (float(net_w)/image_w) < (float(net_h)/image_h):\n",
    "        new_w = net_w\n",
    "        new_h = (image_h*net_w)/image_w\n",
    "    else:\n",
    "        new_h = net_w\n",
    "        new_w = (image_w*net_h)/image_h\n",
    "        \n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        \n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
    "        \n",
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0\n",
    "                    \n",
    "def draw_boxes(image, boxes, labels, obj_thresh):\n",
    "    for box in boxes:\n",
    "        label_str = ''\n",
    "        label = -1\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            if box.classes[i] > obj_thresh:\n",
    "                label_str += labels[i]\n",
    "                label = i\n",
    "                print(labels[i] + ': ' + str(box.classes[i]*100) + '%')\n",
    "                \n",
    "        if label >= 0:\n",
    "            cv2.rectangle(image, (box.xmin,box.ymin), (box.xmax,box.ymax), (0,255,0), 3)\n",
    "            cv2.putText(image, \n",
    "                        label_str + ' ' + str(box.get_score()), \n",
    "                        (box.xmin, box.ymin - 13), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        1e-3 * image.shape[0], \n",
    "                        (0,255,0), 2)\n",
    "        \n",
    "    return image      \n",
    "\n",
    "def _main_(args):\n",
    "    weights_path = args.weights\n",
    "    image_path   = args.image\n",
    "\n",
    "    # set some parameters\n",
    "    net_h, net_w = 416, 416\n",
    "    obj_thresh, nms_thresh = 0.5, 0.45\n",
    "    anchors = [[116,90,  156,198,  373,326],  [30,61, 62,45,  59,119], [10,13,  16,30,  33,23]]\n",
    "    labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n",
    "              \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n",
    "              \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n",
    "              \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n",
    "              \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n",
    "              \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n",
    "              \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n",
    "              \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n",
    "              \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n",
    "              \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "\n",
    "    # make the yolov3 model to predict 80 classes on COCO\n",
    "    yolov3 = make_yolov3_model()\n",
    "\n",
    "    # load the weights trained on COCO into the model\n",
    "    weight_reader = WeightReader(weights_path)\n",
    "    weight_reader.load_weights(yolov3)\n",
    "\n",
    "    # preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    new_image = preprocess_input(image, net_h, net_w)\n",
    "\n",
    "    # run the prediction\n",
    "    yolos = yolov3.predict(new_image)\n",
    "    boxes = []\n",
    "\n",
    "    for i in range(len(yolos)):\n",
    "        # decode the output of the network\n",
    "        boxes += decode_netout(yolos[i][0], anchors[i], obj_thresh, nms_thresh, net_h, net_w)\n",
    "\n",
    "    # correct the sizes of the bounding boxes\n",
    "    correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w)\n",
    "\n",
    "    # suppress non-maximal boxes\n",
    "    do_nms(boxes, nms_thresh)     \n",
    "\n",
    "    # draw bounding boxes on the image using labels\n",
    "    draw_boxes(image, boxes, labels, obj_thresh) \n",
    " \n",
    "    # write the image with bounding boxes to file\n",
    "    cv2.imwrite(image_path[:-4] + '_detected' + image_path[-4:], (image).astype('uint8')) \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = argparser.parse_args()\n",
    "#     _main_(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version1 =  make_yolov3_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {'role':'system','content':'You are a kind helpful assistant'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while True:\n",
    "message = input('User: ')\n",
    "if message :\n",
    "    messages.append(\n",
    "        {'role':'user','content':message},\n",
    "    )\n",
    "    chat = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo',messages=messages\n",
    "    )\n",
    "reply = chat.choices[0].message.content\n",
    "print(f'CG:{reply}')\n",
    "messages.append({'role':'assistant','context':reply})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
